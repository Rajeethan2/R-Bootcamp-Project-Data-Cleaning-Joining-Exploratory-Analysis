---
title: "The Economic Map of Switzerland"
subtitle: "Geographic Distribution, Firm Size, and Return Patterns in the Swiss Performance Index (SPI)"
author: "Valentino Pichler, Rajeethan Ratnasingam"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true
    code_folding: hide
    theme: flatly
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
---

```{r knitr-setup, include=FALSE}
# Set global chunk options
knitr::opts_chunk$set(
  echo = TRUE,           # Show code by default (use code_folding in YAML)
  message = FALSE,       # Suppress messages
  warning = FALSE,       # Suppress warnings
  fig.width = 12,        # Default figure width
  fig.height = 8,        # Default figure height
  fig.align = 'center'   # Center all figures
)

```

```{r R-setup, include=FALSE}
# Set R options for consistent output
options(show.signif.stars = FALSE)
rnd <- 2                            # Default rounding for tables
```

# Introduction and Key Findings

## Client Background

Our client is a U.S.-based portfolio manager seeking to allocate capital to the Swiss equity market. Switzerland is widely regarded as a stable and attractive investment destination; however, the client's current exposure and market knowledge are largely limited to globally visible large-cap firms such as Nestlé, Roche, Novartis, and UBS.

The client is interested in identifying additional investment opportunities beyond these well-known companies. In particular, the focus lies on smaller and less internationally visible Swiss firms, often referred to as "hidden champions", as well as on understanding how investment opportunities are geographically distributed across Swiss cantons.

## Client Assumptions and Hypotheses

Based on prior experience and market perception, the client holds three key assumptions about the Swiss equity market. These assumptions require systematic and data-driven validation.

1. **Firm Size Assumption:** Swiss small-cap firms ("hidden champions") exhibit higher average returns than large-cap firms, as they are less followed by international investors and may be mispriced.

2. **Geographic Location Assumption:** Companies headquartered in the canton of Zug exhibit higher average stock returns than firms located in other Swiss cantons. This assumption is motivated by Zug's favorable tax environment.

3. **Return Concentration Assumption:** Stock returns in Switzerland are unevenly distributed across canton–sector combinations. This geographic and sectoral clustering of returns may create concentration risk for investors pursuing region-focused investment strategies.

## Our Role as Analysts

We act as financial analysts and investment consultants tasked with evaluating the client's assumptions using publicly available market data and reproducible R-based analysis techniques. Our objective is to replace informal beliefs with a clear and evidence-based assessment of return patterns in the Swiss equity market.

## Research Objectives

The analysis follows three core objectives that directly align with the client's assumptions:

1. **Size premium analysis:** To examine whether firm size is related to differences in stock returns across the Swiss equity market.

2. **Zug advantage assessment:** To test whether firms headquartered in the canton of Zug outperform firms in other Swiss cantons.

3. **Concentration risk identification:** To measure geographic and sectoral clustering in stock returns and assess its impact on concentration risk and risk-adjusted performance.

## Hypotheses

- **H~1~**: Swiss small-cap firms offer attractive *risk-adjusted* returns (Sharpe Ratio) compared to large-cap firms. The size premium, if present, must be evaluated in terms of return per unit of risk, not just absolute returns.

- **H~2~**: Firms headquartered in the canton of Zug have higher average stock returns than firms in other Swiss cantons, *even after controlling for sector composition*.

- **H~3~**: Stock returns exhibit significant concentration across canton–sector combinations. We define "Concentration Risk" as *lack of diversification caused by sectoral clustering within individual cantons*: if a canton hosts predominantly one sector, investing in that canton is effectively a bet on that sector's performance.

## Key Findings

The following sections will provide empirical evidence to test these hypotheses and offer investment recommendations based on our findings.


## Methodology Overview

*Why this section matters for the investor:* Before diving into results, we briefly outline the analytical framework. This ensures transparency in how we measure returns, assess risk, and test the client's assumptions.

### Data and Metrics

- **Return metric**: Monthly arithmetic returns computed from adjusted close prices (dividends and splits accounted for)
- **Aggregation**: Median returns per group (canton, sector, size) to reduce outlier influence; equal-weighted portfolio returns for cumulative performance
- **Time horizons**: 1-year, 3-year, and 10-year strict snapshots (only firms with genuine historical data included)
- **Risk metrics**: Annualized volatility (standard deviation × √252), maximum drawdown, and Sharpe Ratio (Mean Return / Volatility)

### Statistical Approach

- **Wilcoxon tests**: Used instead of t-tests because financial returns are typically non-normal. We test for median differences across groups.
- **Linear regression**: To isolate the effect of one variable (e.g., canton) while controlling for others (e.g., sector, size). This is critical for H2, where sector composition could confound location effects.
- **HHI (Herfindahl-Hirschman Index)**: Measures sectoral concentration within each canton. HHI ≥ 2500 = high concentration.

### Analysis Window

- **Primary window**: 2019–2025 (6 years, covers COVID crisis + recovery)
- **Minimum coverage**: Firms must have data for ≥80% of this window to be included in hypothesis testing

This methodology ensures comparability across size buckets, cantons, and sectors.


# Data Sources and Preparation

## Required Packages

In this section, we load all R packages necessary for data manipulation, visualization, statistical modeling, and geospatial analysis.

```{r packages, echo=TRUE, message=FALSE, warning=FALSE}
# Install missing dependencies if needed
if (!require("parallelly", quietly = TRUE)) install.packages("parallelly", repos = "https://cloud.r-project.org", type = "binary")
if (!require("lava", quietly = TRUE)) install.packages("lava", repos = "https://cloud.r-project.org", type = "binary")

# Data manipulation and transformation
library(tidyverse)      # Core tidyverse packages (dplyr, tidyr, ggplot2, etc.)
library(dplyr)          # Data manipulation (filter, select, mutate, etc.)
library(tidyr)          # Data tidying (pivot, separate, unite, etc.)
library(stringr)        # String manipulation
library(stringi)        # Advanced string operations
library(lubridate)      # Date and time manipulation
library(zoo)            # Time series operations (rolling windows, etc.)

# Financial data and analysis
library(tidyquant)      # Financial analysis and tidy quantitative finance
library(quantmod)       # Quantitative financial modeling
library(broom)          # Tidy model outputs
# library(timetk)         # Time series toolkit - commented out as not used

# Visualization
library(ggplot2)        # Grammar of graphics plotting
library(gridExtra)      # Arrange multiple plots
library(scales)         # Scale functions for visualization
library(patchwork)      # Combine plots elegantly
library(viridis)        # Color-blind friendly palettes
if (!require("ggdist", quietly = TRUE)) {
  install.packages("ggdist")
  library(ggdist) 
} 
if (!require("treemapify", quietly = TRUE)) {
  install.packages("treemapify", repos = "https://cloud.r-project.org")
}
library(treemapify)

# Geospatial analysis
library(sf)             # Simple features for spatial data
library(rnaturalearth)  # World map data
library(rnaturalearthdata)  # High-resolution map data

# Tables and reporting
library(knitr)          # Dynamic report generation
library(kableExtra)     # Enhanced table formatting

# Excel file handling
library(readxl)         # Read Excel files (.xlsx)

# Statistics 
if (!require("moments", quietly = TRUE)) {
  install.packages("moments")
  library(moments)      # For skewness, kurtosis calculations
} 

```


## Dataset Overview

This analysis utilizes two primary datasets **from different sources and in different formats**, demonstrating data integration across heterogeneous data origins:

- **Dataset A (Company Master)**: An **Excel file (.xlsx)** containing company-level metadata for all SPI constituents, including **geographic coordinates (Latitude and Longitude)**. This dataset was compiled from multiple sources and serves as the backbone for all joins and analyses.

- **Dataset B (Price Data)**: Daily stock price data obtained **via API from Yahoo Finance** using the `tidyquant` and `quantmod` R packages. This data is retrieved programmatically through HTTP requests to the Yahoo Finance servers, enabling automated updates and reproducibility.

> **Data Format Diversity**: Dataset A is provided as an Excel spreadsheet (.xlsx), while Dataset B is fetched directly from a web API (Yahoo Finance). This demonstrates the ability to work with different data formats and sources in a single analysis pipeline.


## Dataset A: SPI Company Master

### Description

Dataset A contains comprehensive company-level metadata for all 188 SPI (Swiss Performance Index) constituents. This dataset is provided as an **Excel file (.xlsx)** and includes:

- **Company identifiers**: Ticker symbols, company names, ISIN codes
- **Classification data**: Sector and size bucket assignments
- **Geographic information**: Canton, **Latitude**, and **Longitude** coordinates

> **Geographic Coordinates**: The dataset contains precise latitude and longitude coordinates for each company's headquarters location. These coordinates are stored as scaled integers in the source file (e.g., `474789483` for latitude) and are converted to decimal degrees during data preparation using division by 10,000,000 (resulting in `47.4789483°N`). This coordinate transformation enables accurate geospatial visualization on Swiss maps.

### Loading and Initial Inspection

```{r load-dataset-a, echo=TRUE}
# Load the company master dataset
# Using read_delim for semicolon-delimited CSV with UTF-8 encoding
company_master <- read_delim(
  "SPI_clean.csv",
  delim = ";",                        # Semicolon delimiter
  locale = locale(encoding = "UTF-8"), # Handle Swiss special characters
  show_col_types = FALSE              # Suppress column type messages
)

# Display basic information about the dataset
cat("=== COMPANY MASTER DATASET STRUCTURE ===\n")
cat("Number of companies:", nrow(company_master), "\n")
cat("Number of variables:", ncol(company_master), "\n\n")

# Show column names and types
cat("Column names:\n")
print(names(company_master))

# Display first few rows
cat("\n=== FIRST 5 COMPANIES ===\n")
print(head(company_master, 5))
```

**Interpretation:** The dataset contains 188 Swiss companies with 13 variables each. The presence of both ticker symbols and ISIN codes provides multiple ways to link this data with external sources.


### Variable Descriptions

```{r dataset-a-variables, echo=TRUE}
# Create a data dictionary table
data_dictionary <- tribble(
  ~Variable, ~Type, ~Description,
  "Ticker", "Character", "Stock ticker symbol (standardized format)",
  "Name", "Character", "Company name",
  "ISIN", "Character", "International Securities Identification Number",
  "Sector", "Categorical", "Industry sector classification (12 categories)",
  "Size", "Categorical", "Market cap classification: Small, Mid, Large",
  "Zip_Code", "Numeric", "Swiss postal code",
  "City", "Character", "City where company is headquartered",
  "District", "Character", "Administrative district",
  "Canton", "Categorical", "Swiss canton (22 total)",
  "Latitude", "Numeric(Geographic)", "Geographic coordinate (scaled)",
  "Longitude", "Numeric (Geographic)", "Geographic coordinate (scaled)",
  "Full_Address", "Character", "Complete address string",
  "Ticker_Original", "Character", "Original ticker before standardization"
)

# Display as formatted table
kable(data_dictionary, 
      caption = "Table 1: Variable Descriptions for Dataset A",
      align = c("l", "l", "l")) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                full_width = FALSE,
                position = "left")
```

**Interpretation:** The dataset combines financial classification (sector, size) with geographic information (canton, coordinates), enabling spatial analysis of Swiss equity markets. The presence of both categorical and numeric variables allows for diverse analytical approaches.


### Data Quality Assessment

```{r dataset-a-quality, echo=TRUE, fig.width=14, fig.height=8, fig.cap="Figure 1: Data Quality Assessment for Dataset A"}
# Check for missing values
missing_summary <- company_master %>%
  summarise(across(everything(), ~sum(is.na(.)))) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Missing_Count") %>%
  mutate(Missing_Percentage = (Missing_Count / nrow(company_master)) * 100)

cat("=== MISSING VALUES SUMMARY ===\n")
print(missing_summary)

# Sector distribution
sector_dist <- company_master %>%
  count(Sector, name = "Count") %>%
  arrange(desc(Count))

cat("\n=== SECTOR DISTRIBUTION ===\n")
print(sector_dist)

# Size distribution
size_dist <- company_master %>%
  count(Size, name = "Count") %>%
  arrange(desc(Count))

cat("\n=== SIZE DISTRIBUTION ===\n")
print(size_dist)

# Canton distribution (top 10)
canton_dist <- company_master %>%
  count(Canton, name = "Count") %>%
  arrange(desc(Count)) %>%
  head(10)

cat("\n=== TOP 10 CANTONS BY COMPANY COUNT ===\n")
print(canton_dist)

# Create visualization: Sector and Size distribution
p1 <- ggplot(company_master, aes(x = reorder(Sector, Sector, function(x) length(x)))) +
  geom_bar(fill = "#2c7fb8", alpha = 0.8) +
  coord_flip() +
  theme_minimal(base_size = 11) +
  labs(title = "A) Company Count by Sector",
       x = "Sector",
       y = "Number of Companies") +
  theme(plot.title = element_text(face = "bold"))

p2 <- ggplot(company_master, aes(x = Size, fill = Size)) +
  geom_bar(alpha = 0.8) +
  scale_fill_manual(values = c("Large" = "#d7191c", "Mid" = "#fdae61", "Small" = "#2c7bb6")) +
  theme_minimal(base_size = 11) +
  labs(title = "B) Company Count by Size Bucket",
       x = "Size Category",
       y = "Number of Companies") +
  theme(plot.title = element_text(face = "bold"),
        legend.position = "none") +
  geom_text(stat = 'count', aes(label = after_stat(count)), vjust = -0.5, size = 4)

p3 <- ggplot(canton_dist, aes(x = reorder(Canton, Count), y = Count)) +
  geom_col(fill = "#7fcdbb", alpha = 0.8) +
  coord_flip() +
  theme_minimal(base_size = 11) +
  labs(title = "C) Top 10 Cantons by Company Count",
       x = "Canton",
       y = "Number of Companies") +
  theme(plot.title = element_text(face = "bold")) +
  geom_text(aes(label = Count), hjust = -0.3, size = 3.5)

# Combine plots using patchwork
(p1 | p2) / p3 + 
  plot_annotation(
    title = "Dataset A: Distribution Overview",
    subtitle = "Company composition by sector, size, and geographic location",
    theme = theme(plot.title = element_text(size = 16, face = "bold"),
                  plot.subtitle = element_text(size = 12))
  )
```

**Interpretation:** 

- **No missing data**: All 188 companies have complete information across all variables, indicating excellent data quality. The Web Scraping process produced a high quality dataset.

- **Sector concentration**: Industrial Goods dominates with 95 companies (50.5%), followed by Health Care and Banks. This reflects Switzerland's traditional strengths in manufacturing and pharmaceuticals.

- **Size skewness**: The dataset is heavily skewed toward small-cap firms (159 companies, 84.6%), with only 13 large-cap and 16 mid-cap companies. This distribution is typical of broader market indices.

- **Geographic concentration**: Zürich leads with 49 companies (26.1%), followed by Zug and other financial centers. This concentration has implications for portfolio diversification strategies.


## Dataset B: Daily Price Data

### Description

Dataset B consists of daily stock price data for all companies in the master dataset, downloaded from the Yahoo Finance API. This dataset is the foundation for computing financial metrics including:

- Monthly returns panel (primary dataset for hypothesis testing)
- Risk metrics (volatility, drawdowns)
- Risk-adjusted performance ratios
- Cumulative returns over different horizons (1-year, 3-year, 10-year)

### Key Features

The price data pipeline includes:

- **Coverage filtering**: Ensures companies have sufficient data (≥80% of analysis window)
- **Strict snapshot logic**: Returns calculated only with genuine historical data
- **Risk metrics**: Volatility (annualized), maximum drawdown, and risk-adjusted ratios
- **Equal-weight indices**: Portfolio-level returns by size, sector, and canton

    <button type="button" class="collapsible">TEXT in the GRAY BAR</button>
      <div class="content">

```{r setup-ds-B, message=FALSE, warining=FALSE}

################################################################################
# PARAMETERS
################################################################################

PARAMS <- list(
  # Master data file (Excel format - .xlsx)
  master_file = "SPI_tickers_fixed_excel.xlsx",
  
  # Date range for Yahoo downloads
  from_date = "2014-01-01",
  to_date = "2025-12-31",
  
  # Analysis window for monthly panel (coverage filtering)
  analysis_from = "2019-01-01",
  analysis_to = "2025-12-31",
  
  # Coverage threshold (80% = firm must have data for 80% of analysis window)
  min_coverage_ratio = 0.80,
  
  # Cache settings
  use_cache = TRUE,
  cache_file = "data/yahoo_prices_cache.csv",
  
  # Horizons for strict snapshots (years back from to_date)
  horizons_years = c(1, 3, 10),
  
  # Output directory
  output_dir = "data"
)

# Ensure output directory exists
dir.create(PARAMS$output_dir, showWarnings = FALSE, recursive = TRUE)
```

```{r functions, message=FALSE, warning=FALSE}
################################################################################
# FUNCTION: Load and clean master data from semicolon CSV
################################################################################

load_master <- function(filepath) {
  cat("\n=== Loading Master Data ===\n")
  cat("File:", filepath, "\n")
  
  # Read Excel file (.xlsx format)
  # Excel format demonstrates handling multiple data formats in the pipeline
  raw <- read_excel(filepath)
  
  cat("Initial rows:", nrow(raw), "\n")
  cat("Initial columns:", paste(names(raw), collapse = ", "), "\n")
  
  # Standardize column names: snake_case + lowercase
  master <- raw %>%
    rename_with(~ str_replace_all(.x, "\\s+", "_") %>% str_to_lower())
  
  # Expected columns after standardization
  expected_mapping <- c(
    ticker = "tickersymbol",
    name = "companyname",
    size = "size_bucket"
  )
  
  # Rename to standard names
  for (old_name in names(expected_mapping)) {
    new_name <- expected_mapping[old_name]
    if (old_name %in% names(master)) {
      master <- master %>% rename(!!sym(new_name) := !!sym(old_name))
    }
  }
  
  cat("Standardized columns:", paste(names(master), collapse = ", "), "\n")
  
  # Clean and validate required columns using any_of() for robustness
  master <- master %>%
    mutate(
      tickersymbol = str_trim(tickersymbol),
      companyname = str_trim(companyname)
    ) %>%
    # Safely handle optional columns that may not exist
    mutate(across(any_of(c("sector", "canton", "size_bucket")), str_trim)) %>%
    # Ensure optional columns exist as NA if missing
    {if (!"sector" %in% names(.)) mutate(., sector = NA_character_) else .} %>%
    {if (!"canton" %in% names(.)) mutate(., canton = NA_character_) else .} %>%
    {if (!"size_bucket" %in% names(.)) mutate(., size_bucket = NA_character_) else .} %>%
    # Remove rows with missing ticker
    filter(!is.na(tickersymbol), tickersymbol != "") %>%
    # Remove duplicates (keep first occurrence)
    distinct(tickersymbol, .keep_all = TRUE)
  
  # Verify tickers have .SW suffix (should already be present)
  master <- master %>%
    mutate(
      has_sw_suffix = str_detect(tickersymbol, "\\.SW$"),
      tickersymbol = if_else(
        has_sw_suffix,
        tickersymbol,
        paste0(tickersymbol, ".SW")
      )
    ) %>%
    select(-has_sw_suffix)
  
  cat("Cleaned master data:", nrow(master), "firms\n")
  cat("Columns retained:", paste(names(master), collapse = ", "), "\n")
  cat("Sample tickers:", paste(head(master$tickersymbol, 10), collapse = ", "), "\n")
  
  return(master)
}

################################################################################
# FUNCTION: Download Yahoo prices with robust fallbacks
################################################################################

get_yahoo_prices <- function(symbols, from, to, use_cache = FALSE, cache_file = NULL) {
  cat("\n=== Downloading Yahoo Finance Data ===\n")
  cat("Symbols to download:", length(symbols), "\n")
  cat("Date range:", from, "to", to, "\n")
  
  # Check cache
  if (use_cache && !is.null(cache_file) && file.exists(cache_file)) {
    cat("Loading from cache:", cache_file, "\n")
    cached <- read_csv(cache_file, show_col_types = FALSE) %>%
      mutate(date = as.Date(date))
    cat("Cached data loaded:", nrow(cached), "rows for", 
        length(unique(cached$symbol)), "symbols\n")
    return(cached)
  }
  
  # Download fresh data
  all_prices <- list()
  failed_tickers <- character()
  partial_tickers <- character()
  
  pb <- txtProgressBar(min = 0, max = length(symbols), style = 3)
  
  for (i in seq_along(symbols)) {
    sym <- symbols[i]
    setTxtProgressBar(pb, i)
    
    tryCatch({
      # Download using quantmod
      suppressWarnings({
        getSymbols(sym, src = "yahoo", from = from, to = to, 
                   auto.assign = TRUE, warnings = FALSE)
      })
      
      # Get the downloaded data
      df <- get(sym)
      
      if (is.null(df) || nrow(df) == 0) {
        failed_tickers <- c(failed_tickers, sym)
        next
      }
      
      # ROBUST: Extract Adjusted Close with fallback to Close
      adj_col <- grep("Adjusted", names(df), value = TRUE)
      
      if (length(adj_col) == 0) {
        # Fallback to Close
        adj_col <- grep("\\.Close$|Close", names(df), value = TRUE)
        if (length(adj_col) > 0) {
          warning("Symbol ", sym, ": Using Close price (Adjusted not available)")
          partial_tickers <- c(partial_tickers, sym)
        } else {
          failed_tickers <- c(failed_tickers, sym)
          next
        }
      }
      
      # Use first match if multiple
      adj_col <- adj_col[1]
      
      # Extract Volume (optional but preferred)
      vol_col <- grep("\\.Volume$|Volume", names(df), value = TRUE)
      vol_col <- if (length(vol_col) > 0) vol_col[1] else NULL
      
      # Build clean dataframe
      price_df <- data.frame(
        date = index(df),
        symbol = sym,
        adjusted_price = as.numeric(df[, adj_col]),
        stringsAsFactors = FALSE
      )
      
      # Add volume if available
      if (!is.null(vol_col)) {
        price_df$volume <- as.numeric(df[, vol_col])
      } else {
        price_df$volume <- NA_real_
      }
      
      # Handle trailing NAs: use last non-NA value (as-of logic)
      if (is.na(tail(price_df$adjusted_price, 1))) {
        last_valid <- tail(price_df$adjusted_price[!is.na(price_df$adjusted_price)], 1)
        if (length(last_valid) > 0) {
          price_df$adjusted_price[nrow(price_df)] <- last_valid
        }
      }
      
      # Filter out rows with NA prices
      price_df <- price_df %>% filter(!is.na(adjusted_price))
      
      # Check if we have enough data
      if (nrow(price_df) < 20) {
        partial_tickers <- c(partial_tickers, sym)
      }
      
      all_prices[[sym]] <- price_df
      
      # Clean up
      rm(list = sym, envir = .GlobalEnv)
      
    }, error = function(e) {
      failed_tickers <<- c(failed_tickers, sym)
    })
    
    Sys.sleep(0.1) # Rate limiting
  }
  
  close(pb)
  
  # Combine all price data
  if (length(all_prices) > 0) {
    prices <- bind_rows(all_prices)
  } else {
    prices <- data.frame(
      date = as.Date(character()),
      symbol = character(),
      adjusted_price = numeric(),
      volume = numeric()
    )
  }
  
  cat("\nDownload summary:\n")
  cat("  Successfully downloaded:", length(all_prices), "symbols\n")
  cat("  Partial data (warnings):", length(partial_tickers), "symbols\n")
  cat("  Failed:", length(failed_tickers), "symbols\n")
  
  if (length(failed_tickers) > 0) {
    cat("  Failed tickers:", paste(head(failed_tickers, 20), collapse = ", "), "\n")
  }
  
  # Save to cache
  if (!is.null(cache_file)) {
    write_csv(prices, cache_file)
    cat("Cached to:", cache_file, "\n")
  }
  
  # Save failed and partial tickers for report/limitations documentation
  if (length(failed_tickers) > 0) {
    failed_df <- data.frame(
      symbol = failed_tickers,
      reason = "Download failed or no data returned",
      stringsAsFactors = FALSE
    )
    write_csv(failed_df, "data/yahoo_failed_tickers.csv")
    cat("Failed tickers saved to: data/yahoo_failed_tickers.csv\n")
  }
  
  if (length(partial_tickers) > 0) {
    partial_df <- data.frame(
      symbol = partial_tickers,
      reason = "Adjusted price unavailable or insufficient data (<20 rows)",
      stringsAsFactors = FALSE
    )
    write_csv(partial_df, "data/yahoo_partial_tickers.csv")
    cat("Partial tickers saved to: data/yahoo_partial_tickers.csv\n")
  }
  
  return(prices)
}

################################################################################
# FUNCTION: Compute daily features including drawdown
################################################################################

compute_daily_features <- function(price_data) {
  cat("\n=== Computing Daily Features ===\n")
  
  daily_features <- price_data %>%
    group_by(symbol) %>%
    arrange(date) %>%
    mutate(
      # Daily return
      daily_return = adjusted_price / lag(adjusted_price) - 1,
      
      # Cumulative maximum (running peak)
      cum_max = cummax(adjusted_price),
      
      # Drawdown from peak (negative values indicate decline from peak)
      drawdown = (adjusted_price / cum_max) - 1,
      
      # Rolling 20-day volatility (annualized)
      rolling_volatility_20d = rollapply(
        daily_return, 
        width = 20, 
        FUN = function(x) sd(x, na.rm = TRUE) * sqrt(252),
        fill = NA,
        align = "right"
      )
    ) %>%
    ungroup()
  
  cat("Daily features computed:", nrow(daily_features), "rows\n")
  cat("Features added: daily_return, cum_max, drawdown, rolling_volatility_20d\n")
  
  return(daily_features)
}

################################################################################
# FUNCTION: Compute maximum drawdown by period
################################################################################

compute_max_drawdown <- function(price_data, to_date, horizon_years = NULL) {
  to_date <- as.Date(to_date)
  
  # If horizon specified, filter to recent period
  if (!is.null(horizon_years)) {
    from_date <- to_date - years(horizon_years)
    price_data <- price_data %>% 
      filter(date >= from_date, date <= to_date)
  } else {
    price_data <- price_data %>% 
      filter(date <= to_date)
  }
  
  # CRITICAL: Recompute cummax and drawdown WITHIN the filtered window
  # (not using historical drawdown which may reference peaks outside window)
  max_dd <- price_data %>%
    group_by(symbol) %>%
    arrange(date) %>%
    mutate(
      cum_max_window = cummax(adjusted_price),
      drawdown_window = (adjusted_price / cum_max_window) - 1
    ) %>%
    summarize(
      max_drawdown = if_else(
        all(is.na(drawdown_window)),
        NA_real_,
        abs(min(drawdown_window, na.rm = TRUE))
      ),
      .groups = "drop"
    )
  
  return(max_dd)
}

################################################################################
# FUNCTION: Compute monthly returns panel (PRIMARY ANALYSIS DATASET)
################################################################################

compute_monthly_returns <- function(price_data, master_data) {
  cat("\n=== Computing Monthly Returns Panel ===\n")
  
  # Compute monthly returns using tidyquant
  monthly_panel <- price_data %>%
    group_by(symbol) %>%
    tq_transmute(
      select = adjusted_price,
      mutate_fun = periodReturn,
      period = "monthly",
      col_rename = "ret_monthly"
    ) %>%
    ungroup() %>%
    rename(month = date) %>%
    # CRITICAL: Align to month start for consistent joins
    mutate(month = floor_date(month, "month"))
  
  # Add monthly volume (average within month) - NA-safe
  monthly_volume <- price_data %>%
    mutate(month = floor_date(date, "month")) %>%
    group_by(symbol, month) %>%
    summarize(
      n_days_with_volume = sum(!is.na(volume)),
      volume_monthly = if_else(
        all(is.na(volume)),
        NA_real_,
        mean(volume, na.rm = TRUE)
      ),
      .groups = "drop"
    )
  
  # Combine
  monthly_panel <- monthly_panel %>%
    left_join(monthly_volume, by = c("symbol", "month"))
  
  # Join with master metadata
  monthly_panel <- monthly_panel %>%
    left_join(
      master_data,
      by = c("symbol" = "tickersymbol")
    ) %>%
    select(month, symbol, ret_monthly, volume_monthly, 
           companyname, sector, canton, size_bucket, everything())
  
  # Add cumulative returns for portfolio growth visualization
  cat("Computing cumulative returns for portfolio visualization...\n")
  monthly_panel <- monthly_panel %>%
    group_by(symbol) %>%
    arrange(month) %>%
    mutate(
      cum_ret = cumprod(1 + ret_monthly) - 1  # Cumulative return per symbol
    ) %>%
    ungroup()
  
  cat("Monthly panel created:", nrow(monthly_panel), "rows\n")
  cat("Date range:", min(monthly_panel$month), "to", max(monthly_panel$month), "\n")
  cat("Unique symbols:", length(unique(monthly_panel$symbol)), "\n")
  
  return(monthly_panel)
}

################################################################################
# FUNCTION: Compute coverage and filter for analysis
################################################################################

compute_coverage_filter <- function(monthly_panel, analysis_from, analysis_to, 
                                    min_coverage_ratio = 0.80) {
  cat("\n=== Computing Coverage Filter ===\n")
  cat("Analysis window:", analysis_from, "to", analysis_to, "\n")
  cat("Minimum coverage ratio:", min_coverage_ratio, "\n")
  
  # Calculate total possible months in analysis window
  total_months <- interval(as.Date(analysis_from), as.Date(analysis_to)) %/% months(1) + 1
  cat("Total months in window:", total_months, "\n")
  
  # Filter panel to analysis window and ensure unique month-symbol combinations
  panel_window <- monthly_panel %>%
    filter(month >= as.Date(analysis_from), month <= as.Date(analysis_to)) %>%
    group_by(symbol, month) %>%
    slice(1) %>%  # Deduplicate if any duplicates exist
    ungroup()
  
  # Compute coverage per symbol
  coverage_summary <- panel_window %>%
    group_by(symbol) %>%
    summarize(
      months_available = n_distinct(month),
      first_month = min(month),
      last_month = max(month),
      .groups = "drop"
    ) %>%
    mutate(
      coverage_ratio = months_available / total_months,
      is_in_analysis_sample = coverage_ratio >= min_coverage_ratio
    )
  
  # Join company names
  coverage_summary <- coverage_summary %>%
    left_join(
      monthly_panel %>% 
        select(symbol, companyname) %>% 
        distinct(),
      by = "symbol"
    ) %>%
    select(symbol, companyname, everything())
  
  cat("Coverage summary:\n")
  cat("  Total symbols:", nrow(coverage_summary), "\n")
  cat("  Meeting coverage threshold:", sum(coverage_summary$is_in_analysis_sample), "\n")
  cat("  Excluded (insufficient coverage):", 
      sum(!coverage_summary$is_in_analysis_sample), "\n")
  
  # Create filtered analysis panel
  analysis_symbols <- coverage_summary %>%
    filter(is_in_analysis_sample) %>%
    pull(symbol)
  
  monthly_panel_analysis <- panel_window %>%
    filter(symbol %in% analysis_symbols)
  
  cat("Analysis panel:", nrow(monthly_panel_analysis), "rows,", 
      length(unique(monthly_panel_analysis$symbol)), "symbols\n")
  
  # Compute equal-weight group indices for portfolio visualization
  cat("Computing equal-weight group indices (size, sector, canton)...\n")
  
  # Size bucket indices
  size_indices <- monthly_panel_analysis %>%
    filter(!is.na(size_bucket)) %>%
    group_by(month, size_bucket) %>%
    summarize(
      ret_ew = mean(ret_monthly, na.rm = TRUE),
      n_firms = n(),
      .groups = "drop"
    ) %>%
    group_by(size_bucket) %>%
    arrange(month) %>%
    mutate(
      cum_ret_ew = cumprod(1 + ret_ew) - 1
    ) %>%
    ungroup()
  
  # Sector indices
  sector_indices <- monthly_panel_analysis %>%
    filter(!is.na(sector)) %>%
    group_by(month, sector) %>%
    summarize(
      ret_ew = mean(ret_monthly, na.rm = TRUE),
      n_firms = n(),
      .groups = "drop"
    ) %>%
    group_by(sector) %>%
    arrange(month) %>%
    mutate(
      cum_ret_ew = cumprod(1 + ret_ew) - 1
    ) %>%
    ungroup()
  
  # Canton indices
  canton_indices <- monthly_panel_analysis %>%
    filter(!is.na(canton)) %>%
    group_by(month, canton) %>%
    summarize(
      ret_ew = mean(ret_monthly, na.rm = TRUE),
      n_firms = n(),
      .groups = "drop"
    ) %>%
    group_by(canton) %>%
    arrange(month) %>%
    mutate(
      cum_ret_ew = cumprod(1 + ret_ew) - 1
    ) %>%
    ungroup()
  
  cat("Equal-weight indices computed for visualization\n")
  
  return(list(
    coverage_summary = coverage_summary,
    monthly_panel_analysis = monthly_panel_analysis,
    size_indices = size_indices,
    sector_indices = sector_indices,
    canton_indices = canton_indices
  ))
}

################################################################################
# FUNCTION: Compute firm summary with strict snapshots and risk metrics
################################################################################

compute_firm_summary_strict <- function(price_data, daily_features, master_data, 
                                        to_date, horizons_years) {
  cat("\n=== Computing Firm Summary with Risk Metrics ===\n")
  cat("As-of date (last trading day <=):", to_date, "\n")
  cat("Horizons (years back):", paste(horizons_years, collapse = ", "), "\n")
  
  to_date <- as.Date(to_date)
  
  # For each symbol, compute returns at specified horizons
  firm_summaries <- list()
  symbols <- unique(price_data$symbol)
  
  for (sym in symbols) {
    sym_data <- price_data %>%
      filter(symbol == sym) %>%
      arrange(date)
    
    # Get end price (last available <= to_date)
    end_data <- sym_data %>% filter(date <= to_date)
    if (nrow(end_data) == 0) next
    
    # NA-safe extraction: use last non-NA price
    end_prices_valid <- na.omit(end_data$adjusted_price)
    if (length(end_prices_valid) == 0) next
    
    end_price <- tail(end_prices_valid, 1)
    end_date <- tail(end_data$date, 1)
    
    # Initialize summary
    summary_row <- data.frame(
      symbol = sym,
      as_of_date = end_date,
      end_price = end_price,
      stringsAsFactors = FALSE
    )
    
    # Compute returns for each horizon (STRICT: require sufficient history)
    for (h in horizons_years) {
      horizon_date <- end_date - years(h)
      
      # Get start price (first available >= horizon_date)
      start_data <- sym_data %>% filter(date >= horizon_date)
      
      if (nrow(start_data) > 0) {
        # NA-safe extraction: use first non-NA price
        start_prices_valid <- na.omit(start_data$adjusted_price)
        
        if (length(start_prices_valid) > 0) {
          start_price <- head(start_prices_valid, 1)
          # Get corresponding date (may not be exact first row if NA)
          start_date <- start_data$date[which(!is.na(start_data$adjusted_price))[1]]
          
          # CRITICAL: Check if firm has REAL h-year history
          # Tolerance: 90 days (3 months) from theoretical horizon_date
          # If start_date is too far from horizon_date, history is insufficient
          days_from_horizon <- as.numeric(start_date - horizon_date)
          
          if (days_from_horizon <= 90) {
            # Firm has sufficient history - calculate return
            ret <- (end_price / start_price) - 1
            
            # Calculate CAGR
            years_actual <- as.numeric(end_date - start_date) / 365.25
            cagr <- if (years_actual > 0) {
              (1 + ret)^(1 / years_actual) - 1
            } else {
              NA_real_
            }
            
            summary_row[[paste0("ret_", h, "y_strict")]] <- ret
            summary_row[[paste0("cagr_", h, "y")]] <- cagr
            summary_row[[paste0("start_date_", h, "y")]] <- start_date
          } else {
            # Insufficient history - set to NA
            summary_row[[paste0("ret_", h, "y_strict")]] <- NA_real_
            summary_row[[paste0("cagr_", h, "y")]] <- NA_real_
            summary_row[[paste0("start_date_", h, "y")]] <- NA
          }
        } else {
          summary_row[[paste0("ret_", h, "y_strict")]] <- NA_real_
          summary_row[[paste0("cagr_", h, "y")]] <- NA_real_
          summary_row[[paste0("start_date_", h, "y")]] <- NA
        }
      } else {
        summary_row[[paste0("ret_", h, "y_strict")]] <- NA_real_
        summary_row[[paste0("cagr_", h, "y")]] <- NA_real_
        summary_row[[paste0("start_date_", h, "y")]] <- NA
      }
    }
    
    firm_summaries[[sym]] <- summary_row
  }
  
  firm_summary <- bind_rows(firm_summaries)
  
  # Compute volatility metrics: ALL-PERIOD and 3-YEAR (horizon-consistent)
  cat("Computing volatility metrics (all-period and 3-year)...\n")
  
  # All-period volatility
  volatility_all <- daily_features %>%
    filter(date <= to_date) %>%
    group_by(symbol) %>%
    summarize(
      n_obs_all = sum(!is.na(daily_return)),
      volatility_pa_all = sd(daily_return, na.rm = TRUE) * sqrt(252),
      .groups = "drop"
    )
  
  # 3-year volatility (restrict to last 3 years, horizon-consistent)
  # NOTE: Firms with short listing history may have NA or limited data
  three_years_ago <- to_date - years(3)
  volatility_3y <- daily_features %>%
    filter(date >= three_years_ago, date <= to_date) %>%
    group_by(symbol) %>%
    summarize(
      n_obs_3y = sum(!is.na(daily_return)),
      volatility_pa_3y = sd(daily_return, na.rm = TRUE) * sqrt(252),
      .groups = "drop"
    )
  
  # 1-year volatility (for horizon-consistent 1Y risk metrics)
  one_year_ago <- to_date - years(1)
  volatility_1y <- daily_features %>%
    filter(date >= one_year_ago, date <= to_date) %>%
    group_by(symbol) %>%
    summarize(
      n_obs_1y = sum(!is.na(daily_return)),
      volatility_pa_1y = sd(daily_return, na.rm = TRUE) * sqrt(252),
      .groups = "drop"
    )
  
  # Join volatilities
  firm_summary <- firm_summary %>%
    left_join(volatility_all, by = "symbol") %>%
    left_join(volatility_3y, by = "symbol") %>%
    left_join(volatility_1y, by = "symbol")
  
  # Compute max drawdown metrics
  cat("Computing max drawdown metrics (3-year and all-period)...\n")
  
  max_dd_3y <- compute_max_drawdown(daily_features, to_date, horizon_years = 3)
  max_dd_all <- compute_max_drawdown(daily_features, to_date, horizon_years = NULL)
  
  firm_summary <- firm_summary %>%
    left_join(
      max_dd_3y %>% rename(max_drawdown_3y = max_drawdown),
      by = "symbol"
    ) %>%
    left_join(
      max_dd_all %>% rename(max_drawdown_all = max_drawdown),
      by = "symbol"
    )
  
  # COMPUTE RISK-ADJUSTED PERFORMANCE METRICS (horizon-consistent)
  cat("Computing risk-adjusted performance metrics...\n")
  
  # Horizon-consistent risk-adjusted metrics
  firm_summary <- firm_summary %>%
    mutate(
      # Return-to-risk ratios (1Y - horizon-consistent)
      return_to_risk_1y = ret_1y_strict / volatility_pa_1y,
      cagr_to_risk_1y = cagr_1y / volatility_pa_1y,
      
      # Return-to-risk ratios (3Y - horizon-consistent)
      return_to_risk_3y = ret_3y_strict / volatility_pa_3y,
      cagr_to_risk_3y = cagr_3y / volatility_pa_3y,
      
      # Return-to-risk ratios (10Y using all-period vol)
      return_to_risk_10y = ret_10y_strict / volatility_pa_all,
      cagr_to_risk_10y = cagr_10y / volatility_pa_all
    )
  
  # Join with master data
  firm_summary <- firm_summary %>%
    left_join(
      master_data,
      by = c("symbol" = "tickersymbol")
    ) %>%
    select(symbol, companyname, sector, canton, size_bucket, everything())
  
  cat("Firm summary computed for", nrow(firm_summary), "symbols\n")
  cat("Risk metrics included: volatility_pa (all + 3y + 1y), max_drawdown (all + 3y)\n")
  cat("Sample size tracking: n_obs (all + 3y + 1y) for data quality assessment\n")
  cat("Risk-adjusted metrics: return_to_risk, cagr_to_risk (1y/3y/10y, horizon-consistent)\n")
  
  return(firm_summary)
}

################################################################################
# FUNCTION: Create daily joined dataset
################################################################################

create_daily_joined <- function(daily_features, master_data) {
  cat("\n=== Creating Daily Joined Dataset ===\n")
  
  # Join with master
  daily_joined <- daily_features %>%
    left_join(
      master_data,
      by = c("symbol" = "tickersymbol")
    ) %>%
    select(date, symbol, companyname, sector, canton, size_bucket,
           adjusted_price, volume, daily_return, drawdown, cum_max, 
           rolling_volatility_20d, everything())
  
  cat("Daily joined dataset:", nrow(daily_joined), "rows\n")
  
  return(daily_joined)
}
```

```{r run-pipeline}
################################################################################
# MAIN PIPELINE EXECUTION
################################################################################

run_pipeline <- function() {
  cat("\n")
  cat("################################################################################\n")
  cat("# SPI YAHOO FINANCE PIPELINE - EXECUTION\n")
  cat("# Focus: Stock Returns Analysis with Risk Metrics\n")
  cat("################################################################################\n")
  
  # Step 1: Load master data from semicolon CSV
  master <- load_master(PARAMS$master_file)
  
  # Step 2: Download Yahoo prices
  prices_raw <- get_yahoo_prices(
    symbols = master$tickersymbol,
    from = PARAMS$from_date,
    to = PARAMS$to_date,
    use_cache = PARAMS$use_cache,
    cache_file = PARAMS$cache_file
  )
  
  # Save raw prices
  write_csv(prices_raw, file.path(PARAMS$output_dir, "yahoo_prices_raw.csv"))
  cat("\nSaved:", file.path(PARAMS$output_dir, "yahoo_prices_raw.csv"), "\n")
  
  # Step 3: Compute daily features (returns, drawdown)
  daily_features <- compute_daily_features(prices_raw)
  
  # Step 4: Create daily joined dataset
  daily_joined <- create_daily_joined(daily_features, master)
  write_csv(daily_joined, file.path(PARAMS$output_dir, "spi_joined_daily.csv"))
  cat("Saved:", file.path(PARAMS$output_dir, "spi_joined_daily.csv"), "\n")
  
  # Step 5: Compute monthly returns panel (PRIMARY DATASET)
  monthly_panel <- compute_monthly_returns(prices_raw, master)
  write_csv(monthly_panel, file.path(PARAMS$output_dir, "spi_monthly_panel.csv"))
  cat("Saved:", file.path(PARAMS$output_dir, "spi_monthly_panel.csv"), "\n")
  
  # Step 6: Compute coverage and create analysis-ready dataset
  coverage_results <- compute_coverage_filter(
    monthly_panel,
    PARAMS$analysis_from,
    PARAMS$analysis_to,
    PARAMS$min_coverage_ratio
  )
  
  write_csv(coverage_results$coverage_summary, 
            file.path(PARAMS$output_dir, "spi_coverage_summary.csv"))
  cat("Saved:", file.path(PARAMS$output_dir, "spi_coverage_summary.csv"), "\n")
  
  write_csv(coverage_results$monthly_panel_analysis, 
            file.path(PARAMS$output_dir, "spi_monthly_panel_analysis.csv"))
  cat("Saved:", file.path(PARAMS$output_dir, "spi_monthly_panel_analysis.csv"), "\n")
  
  # Save equal-weight group indices for portfolio visualization
  write_csv(coverage_results$size_indices, 
            file.path(PARAMS$output_dir, "spi_size_indices.csv"))
  cat("Saved:", file.path(PARAMS$output_dir, "spi_size_indices.csv"), "\n")
  
  write_csv(coverage_results$sector_indices, 
            file.path(PARAMS$output_dir, "spi_sector_indices.csv"))
  cat("Saved:", file.path(PARAMS$output_dir, "spi_sector_indices.csv"), "\n")
  
  write_csv(coverage_results$canton_indices, 
            file.path(PARAMS$output_dir, "spi_canton_indices.csv"))
  cat("Saved:", file.path(PARAMS$output_dir, "spi_canton_indices.csv"), "\n")
  
  # Step 7: Compute firm summary with strict snapshots and risk metrics
  firm_summary <- compute_firm_summary_strict(
    prices_raw,
    daily_features,
    master,
    PARAMS$to_date,
    PARAMS$horizons_years
  )
  
  write_csv(firm_summary, file.path(PARAMS$output_dir, "spi_firm_summary_strict.csv"))
  cat("Saved:", file.path(PARAMS$output_dir, "spi_firm_summary_strict.csv"), "\n")
  
  # Final summary
  cat("\n")
  cat("################################################################################\n")
  cat("# PIPELINE COMPLETION SUMMARY\n")
  cat("################################################################################\n")
  cat("Master data:\n")
  cat("  Total firms in master:", nrow(master), "\n")
  cat("\nYahoo Finance download:\n")
  cat("  Firms with price data:", length(unique(prices_raw$symbol)), "\n")
  cat("  Failed downloads:", nrow(master) - length(unique(prices_raw$symbol)), "\n")
  cat("\nMonthly panel analysis:\n")
  cat("  Total monthly observations:", nrow(monthly_panel), "\n")
  cat("  Analysis sample size (coverage >= ", PARAMS$min_coverage_ratio, "):", 
      sum(coverage_results$coverage_summary$is_in_analysis_sample), "\n")
  cat("  Analysis panel observations:", nrow(coverage_results$monthly_panel_analysis), "\n")
  cat("\nFirm summary (snapshots):\n")
  cat("  Firms with snapshot data:", nrow(firm_summary), "\n")
  cat("\nRisk metrics computed:\n")
  cat("  - Volatility (annualized): all-period, 3-year, and 1-year\n")
  cat("  - Max drawdown: all-period and 3-year\n")
  cat("  - Risk-adjusted performance: return/vol and CAGR/vol ratios (horizon-consistent)\n")
  cat("  - Data quality tracking: n_obs for each volatility metric\n")
  cat("\n")
  cat("KEY USAGE NOTES:\n")
  cat("  1. Use 'spi_monthly_panel_analysis.csv' for H1/H2 hypothesis testing\n")
  cat("  2. Use 'spi_firm_summary_strict.csv' for Hidden Champions & risk visuals\n")
  cat("  3. Max drawdown (max_drawdown_3y) shows worst peak-to-trough decline in 3Y window\n")
  cat("  4. Risk-adjusted metrics enable performance comparison across vol profiles\n")
  cat("  5. Check n_obs_3y >= 200 for reliable volatility estimates\n")
  cat("  6. Remember: RETURNS = stock price changes, NOT accounting profitability\n")
  cat("################################################################################\n")
  
  # Return key objects for further analysis
  return(list(
    master = master,
    prices_raw = prices_raw,
    daily_features = daily_features,
    daily_joined = daily_joined,
    monthly_panel = monthly_panel,
    monthly_panel_analysis = coverage_results$monthly_panel_analysis,
    coverage_summary = coverage_results$coverage_summary,
    firm_summary = firm_summary,
    size_indices = coverage_results$size_indices,
    sector_indices = coverage_results$sector_indices,
    canton_indices = coverage_results$canton_indices
  ))
}

################################################################################
# EXECUTE PIPELINE
################################################################################

# Run the pipeline and store results
results <- run_pipeline()

# Optional: Quick exploratory checks
cat("\n=== Quick Data Checks ===\n")

cat("\nMonthly panel (analysis sample) - first rows:\n")
print(head(results$monthly_panel_analysis %>% 
             select(month, symbol, companyname, ret_monthly, sector, canton)))

cat("\nFirm summary - sample with risk metrics:\n")
print(head(results$firm_summary %>% 
             select(symbol, companyname, ret_3y_strict, cagr_3y, 
                    volatility_pa_3y, max_drawdown_3y, return_to_risk_3y)))

cat("\nCoverage distribution:\n")
print(summary(results$coverage_summary$coverage_ratio))

cat("\nSector distribution:\n")
print(table(results$master$sector))

cat("\nSize bucket distribution:\n")
print(table(results$master$size_bucket))
cat("\n=== Equal-Weight Group Indices (Portfolio Growth) ===\n")
cat("Size indices - sample rows:\n")
print(head(results$size_indices %>% 
             select(month, size_bucket, ret_ew, cum_ret_ew, n_firms)))

cat("\nCanton indices - top 5 cantons (by avg firms):\n")
top_cantons <- results$canton_indices %>%
  group_by(canton) %>%
  summarize(avg_firms = mean(n_firms, na.rm = TRUE), .groups = "drop") %>%
  arrange(desc(avg_firms)) %>%
  head(5) %>%
  pull(canton)
print(results$canton_indices %>% 
        filter(canton %in% top_cantons, month >= max(month) - months(5)) %>%
        select(month, canton, ret_ew, cum_ret_ew, n_firms))
cat("\n=== Risk Metrics Availability ===\n")
cat("Firms with non-NA max_drawdown_3y:", 
    sum(!is.na(results$firm_summary$max_drawdown_3y)), "/", nrow(results$firm_summary), "\n")
cat("Firms with non-NA max_drawdown_all:", 
    sum(!is.na(results$firm_summary$max_drawdown_all)), "/", nrow(results$firm_summary), "\n")
cat("Firms with non-NA volatility_pa_1y:", 
    sum(!is.na(results$firm_summary$volatility_pa_1y)), "/", nrow(results$firm_summary), "\n")
cat("Firms with non-NA volatility_pa_3y:", 
    sum(!is.na(results$firm_summary$volatility_pa_3y)), "/", nrow(results$firm_summary), "\n")
cat("Firms with non-NA volatility_pa_all:", 
    sum(!is.na(results$firm_summary$volatility_pa_all)), "/", nrow(results$firm_summary), "\n")

cat("\n=== Data Quality Metrics ===\n")
cat("Median n_obs_1y (1Y trading days):", 
    round(median(results$firm_summary$n_obs_1y, na.rm = TRUE)), "\n")
cat("Median n_obs_3y (3Y trading days):", 
    round(median(results$firm_summary$n_obs_3y, na.rm = TRUE)), "\n")
cat("Firms with n_obs_3y >= 200 (recommended for stable vol):", 
    sum(results$firm_summary$n_obs_3y >= 200, na.rm = TRUE), "/", nrow(results$firm_summary), "\n")

################################################################################
# END OF SCRIPT
################################################################################
# 
# VISUALIZATION SUGGESTIONS FOR BOOTCAMP REPORT:
# 
# 1. HIDDEN CHAMPIONS ANALYSIS:
#    - Bubble chart: x=volatility_pa_3y, y=cagr_3y, size=market_cap, color=sector
#    - Highlight firms with high cagr_to_risk_3y (>1.5 or top quartile)
#    - Show canton/sector averages as reference points
# 
# 2. CLUSTER RISK ANALYSIS:
#    - Box plot: max_drawdown_3y by sector/canton
#    - Scatter: ret_3y_strict vs max_drawdown_3y (risk-return tradeoff)
#    - Identify sectors/cantons with high drawdown concentration
# 
# 3. HYPOTHESIS TESTING (H1/H2):
#    - Use monthly_panel_analysis for t-tests
#    - Box plots of ret_monthly by canton/sector
#    - Time series of average monthly returns by group
# 
# 4. SIMPLE LINEAR MODELS (optional):
#    - lm(ret_monthly ~ canton + sector + size_bucket)
#    - lm(cagr_3y ~ volatility_pa_3y + max_drawdown_3y + sector)
# 
################################################################################

```

     </p>
    </div>


## Data Cleaning and Preparation

### Data Cleaning Steps

```{r data-cleaning, echo=TRUE}
# ============================================================================
# STEP 1: STANDARDIZE COLUMN NAMES
# ============================================================================
# Convert all column names to lowercase snake_case for consistency

company_master_clean <- company_master %>%
  # Rename columns to standard format (only rename columns that exist)
  rename(
    ticker = Ticker,
    name = Name,
    isin = ISIN,
    sector = Sector,
    size_bucket = Size,
    zip_code = Zip_Code,
    city = City,
    district = District,
    canton = Canton,
    latitude = Latitude,
    longitude = Longitude,
    full_address = Full_Address
  )

# Add ticker_original column if it doesn't exist
if (!"Ticker_Original" %in% names(company_master)) {
  company_master_clean <- company_master_clean %>%
    mutate(ticker_original = ticker)
} else {
  company_master_clean <- company_master_clean %>%
    rename(ticker_original = Ticker_Original)
}

cat("Column names standardized to lowercase snake_case\n")

# ============================================================================
# STEP 2: CLEAN STRING VARIABLES
# ============================================================================
# Remove extra whitespace and standardize text fields

# Standardize canton names (ASCII & English replacements)
standardise_canton_english <- function(x) {
  x %>%
    as.character() %>%
    enc2utf8() %>%
    str_squish() %>%
    stringi::stri_trans_general("Any-Latin; Latin-ASCII") %>%
    str_to_title() %>%
    str_replace_all("^Zurich$", "Zurich") %>%
    str_replace_all("^Geneve$", "Geneva") %>%
    str_replace_all("^Graubunden$", "Grisons") %>%
    str_replace_all("^Luzern$", "Lucerne") %>%
    str_replace_all("^Neuchatel$", "Neuchatel")
}


company_master_clean <- company_master_clean %>%
  mutate(
    # Trim whitespace from all character columns
    across(where(is.character), str_trim),
    # Ensure consistent capitalization for categorical variables
    sector = str_to_title(sector),
    size_bucket = str_to_title(size_bucket),
    # Standardize canton names
    canton = standardise_canton_english(canton)
  )

cat("String variables cleaned (whitespace removed, capitalization standardized)\n")

# ============================================================================
# STEP 3: HANDLE COORDINATE SCALING
# ============================================================================
# Latitude and Longitude are encoded in different scales (1e6 or 1e7)
# Need to convert to standard decimal degrees

company_master_clean <- company_master_clean %>%
  mutate(
    # Convert coordinates to decimal degrees
    # Check scale: if value > 1e6, divide by appropriate factor
    latitude = case_when(
      abs(latitude) > 1e6 ~ latitude / 1e7,  # Scale 1e7 format
      abs(latitude) > 50 ~ latitude / 1e6,    # Scale 1e6 format
      TRUE ~ latitude                          # Already in correct scale
    ),
    longitude = case_when(
      abs(longitude) > 1e6 ~ longitude / 1e7,
      abs(longitude) > 50 ~ longitude / 1e6,
      TRUE ~ longitude
    ),
    # Final validation: Swiss coordinates should be in specific ranges
    # Latitude: ~45.8 to ~47.8, Longitude: ~6.0 to ~10.5
    latitude = ifelse(latitude < 45 | latitude > 48, NA_real_, latitude),
    longitude = ifelse(longitude < 6 | longitude > 11, NA_real_, longitude)
  )

cat("Geographic coordinates converted to decimal degrees\n")
cat(sprintf("  - Latitude range: %.4f to %.4f\n", 
            min(company_master_clean$latitude, na.rm = TRUE),
            max(company_master_clean$latitude, na.rm = TRUE)))
cat(sprintf("  - Longitude range: %.4f to %.4f\n", 
            min(company_master_clean$longitude, na.rm = TRUE),
            max(company_master_clean$longitude, na.rm = TRUE)))

# ============================================================================
# STEP 4: CREATE FACTOR VARIABLES
# ============================================================================
# Convert categorical variables to factors with meaningful level ordering

company_master_clean <- company_master_clean %>%
  mutate(
    # Size: ordered factor (Small < Mid < Large)
    size_bucket = factor(size_bucket, 
                        levels = c("Small", "Mid", "Large"),
                        ordered = TRUE),
    # Sector: factor (unordered, but keep alphabetical)
    sector = factor(sector),
    # Canton: factor
    canton = factor(canton)
  )

cat("Categorical variables converted to factors\n")
cat(sprintf("  - Sectors: %d unique values\n", nlevels(company_master_clean$sector)))
cat(sprintf("  - Cantons: %d unique values\n", nlevels(company_master_clean$canton)))

# ============================================================================
# STEP 5: VALIDATE TICKER FORMAT
# ============================================================================
# Ensure all tickers have the .SW suffix (Swiss exchange identifier)

company_master_clean <- company_master_clean %>%
  mutate(
    # Add .SW suffix if not present
    ticker = ifelse(str_detect(ticker, "\\.SW$"), 
                   ticker, 
                   paste0(ticker, ".SW"))
  )

cat("Ticker symbols validated (all have .SW suffix)\n")

# ============================================================================
# STEP 6: CHECK FOR DUPLICATES
# ============================================================================
# Ensure no duplicate companies in the dataset

duplicates <- company_master_clean %>%
  group_by(ticker) %>%
  filter(n() > 1) %>%
  ungroup()

if (nrow(duplicates) > 0) {
  cat("WARNING: Found", nrow(duplicates), "duplicate ticker(s):\n")
  print(duplicates %>% select(ticker, name))
  # Remove duplicates, keeping first occurrence
  company_master_clean <- company_master_clean %>%
    distinct(ticker, .keep_all = TRUE)
  cat("Duplicates removed, kept first occurrence\n")
} else {
  cat("No duplicate companies found\n")
}

# ============================================================================
# SUMMARY OF CLEANED DATA
# ============================================================================
cat("\n=== DATA CLEANING COMPLETE ===\n")
cat("Final dataset dimensions:", nrow(company_master_clean), "rows,", 
    ncol(company_master_clean), "columns\n")
cat("Companies with valid coordinates:", 
    sum(!is.na(company_master_clean$latitude) & !is.na(company_master_clean$longitude)), "\n")
```

**Interpretation:** The data cleaning process ensures:

1. **Consistency**: All column names follow a standard naming convention
2. **Quality**: String variables are trimmed and properly formatted
3. **Validity**: Geographic coordinates are converted to decimal degrees and validated
4. **Structure**: Categorical variables are properly encoded as factors
5. **Uniqueness**: No duplicate companies remain in the dataset

These steps are essential for reliable analysis and prevent common data quality issues from affecting downstream results.


### Feature Engineering

```{r feature-engineering, echo=TRUE}
# ============================================================================
# STEP 1: GEOGRAPHIC FEATURES
# ============================================================================
# Create additional geographic groupings and classifications

company_master_clean <- company_master_clean %>%
  mutate(
    # Major economic region classification
    region = case_when(
      canton %in% c("Zurich", "Zug", "Schwyz") ~ "Central Switzerland",
      canton %in% c("Geneva", "Vaud", "Valais") ~ "Lake Geneva Region",
      canton %in% c("Basel") ~ "Basel Region",
      canton %in% c("Bern", "Solothurn", "Fribourg") ~ "Mittelland",
      TRUE ~ "Other"
    ),
    # Flag for financial centers (cantons with major stock exchanges)
    is_financial_center = canton %in% c("Zurich", "Geneva", "Basel"),
    # Flag for tax-advantaged canton (Zug hypothesis)
    is_zug = canton == "Zug"
  )

cat("✓ Geographic features created:\n", "  - region: Major economic regions (5 categories)\n", "  - is_financial_center: Binary indicator for financial hubs\n", "  - is_zug: Binary indicator for Zug canton\n\n")

# Show region distribution
cat("Regional distribution:\n")
print(table(company_master_clean$region))

# ============================================================================
# STEP 2: SECTOR AGGREGATION
# ============================================================================
# Create broader sector categories for easier interpretation

company_master_clean <- company_master_clean %>%
  mutate(
    sector_broad = case_when(
      sector %in% c("Banks", "Financial Services", "Insurance") ~ "Financials",
      sector %in% c("Health Care", "Chemicals") ~ "Healthcare & Life Sciences",
      sector %in% c("Industrial Goods", "Technology") ~ "Industrials & Tech",
      sector %in% c("Consumer Products", "Food & Beverage") ~ "Consumer",
      TRUE ~ "Other"
    )
  )

cat("\n✓ Broad sector categories created:\n")
print(table(company_master_clean$sector_broad))

# ============================================================================
# STEP 3: SIZE-RELATED FEATURES
# ============================================================================
# Create numeric size indicators for regression analysis

company_master_clean <- company_master_clean %>%
  mutate(
    # Numeric size indicator (1 = Small, 2 = Mid, 3 = Large)
    size_numeric = as.numeric(size_bucket),
    # Binary indicators for each size category
    is_small_cap = size_bucket == "Small",
    is_mid_cap = size_bucket == "Mid",
    is_large_cap = size_bucket == "Large"
  )

cat("\n✓ Size-related features created:\n")
cat("  - size_numeric: Numeric encoding (1-3)\n")
cat("  - is_small_cap / is_mid_cap / is_large_cap: Binary indicators\n")

# ============================================================================
# STEP 4: CANTON-SECTOR INTERACTION
# ============================================================================
# Create combined canton-sector identifier for clustering analysis

company_master_clean <- company_master_clean %>%
  mutate(
    canton_sector = paste(canton, sector, sep = "_")
  )

cat("\n✓ Interaction features created:\n")
cat(sprintf("  - canton_sector: %d unique combinations\n", 
            n_distinct(company_master_clean$canton_sector)))

# ============================================================================
# STEP 5: COMPANY COUNT FEATURES
# ============================================================================
# Add features about company density in canton and sector

company_master_clean <- company_master_clean %>%
  group_by(canton) %>%
  mutate(companies_in_canton = n()) %>%
  ungroup() %>%
  group_by(sector) %>%
  mutate(companies_in_sector = n()) %>%
  ungroup() %>%
  group_by(canton_sector) %>%
  mutate(companies_in_canton_sector = n()) %>%
  ungroup()

cat("\n✓ Company density features created:\n")
cat("  - companies_in_canton: Number of companies per canton\n")
cat("  - companies_in_sector: Number of companies per sector\n")
cat("  - companies_in_canton_sector: Number in canton-sector combinations\n")

# ============================================================================
# STEP 6: CANTON NAMES NORMALIZATION
# ============================================================================
# Convert all canton name variants to standard lowercase ASCII format

normalize_canton_names <- function(x) {
  x %>%
    as.character() %>%
    enc2utf8() %>%
    str_squish() %>%
    stri_trans_general("Any-Latin; Latin-ASCII") %>%
    str_replace_all("[^\\x20-\\x7E]", "") %>%
    str_to_lower() %>%
    # Standardize common variants
    str_replace("^basel-(stadt|landschaft)$", "basel") %>%
    str_replace("^genve$", "geneva") %>%
    str_replace("^geneva$", "geneva") %>%
    str_replace("^zrich$", "zurich") %>%
    str_replace("^neuchtel$", "neuchatel") %>%
    str_replace("^graubnden$", "grisons") %>%
    str_replace("graubndengrischnnigrigioni", "grisons") %>%
    str_replace("grisons", "grisons") %>%
    str_replace("valaiswallis", "valais") %>%
    str_replace("^lucerne$", "lucerne") %>%
    str_replace("^luzern$", "lucerne")
}

# Prepare company location data: create a canton_key for further analysis
company_geo <- company_master_clean %>%
  mutate(canton_key = normalize_canton_names(canton))

# ============================================================================
# STEP 7: CALCULATE SECTOR VOLUME BY CANTON-SECTOR
# ============================================================================
# Calculate cumulative volume by canton-sector
# Load daily prices with volume

prices <- read_csv("data/yahoo_prices_raw.csv") %>%
  mutate(date = as.Date(date))

# Choose a window: here it is 5 years (WHY??? müssen wir noch erklären)
end_date   <- max(prices$date, na.rm = TRUE)
start_date <- end_date - years(5)

# Calculate the annualized volume: use the average daily volume and multiplying it with the standard number of trading days per year on most stock exchanges
vol_annualized <- prices %>%
  filter(date >= start_date, date <= end_date) %>%
  group_by(symbol) %>%
  summarise(
    trading_days_window = sum(!is.na(volume)),
    avg_daily_volume = mean(volume, na.rm = TRUE),
    annualized_volume = avg_daily_volume * 252, 
    .groups = "drop"
  )

# Left join the master data set with the calculated values 
company_master_vol_annualized <- company_master_clean %>%
  left_join(vol_annualized, by = c("ticker" = "symbol"))

# Group by canton and sector and calculate the cumulative volume
canton_sector_volume <- company_master_vol_annualized %>%
  filter(!is.na(canton), !is.na(sector)) %>%
  group_by(canton, sector) %>%
  summarise(cum_vol = sum(annualized_volume, na.rm = TRUE), .groups = "drop")

# Identify top sector by volume in each canton
canton_top_sector_vol <- canton_sector_volume %>%
  group_by(canton) %>%
  slice_max(order_by = cum_vol, n = 1, with_ties = FALSE) %>%
  ungroup() %>%
  rename(top_sector_vol = sector)

# ============================================================================
# STEP 8: CALCULATE MONTHLY RETURNS PANEL
# ============================================================================
# Calculate monthly returns for each company

# Create month-end adjusted prices per symbol
monthly_prices <- prices %>%
  mutate(date = as.Date(date),
         month = lubridate::floor_date(date, "month")) %>%
  filter(!is.na(adjusted_price)) %>%
  arrange(symbol, date) %>%
  group_by(symbol, month) %>%
  summarise(
    adjusted_price_eom = dplyr::last(adjusted_price),
    .groups = "drop"
  )

# Compute monthly returns from month-end prices
monthly_returns <- monthly_prices %>%
  group_by(symbol) %>%
  arrange(month) %>%
  mutate(ret_monthly = (adjusted_price_eom / lag(adjusted_price_eom) - 1)) %>%
  ungroup() %>%
  filter(!is.na(ret_monthly))


# Join with company master to get size, sector, canton info
monthly_returns <- monthly_returns %>%
  left_join(
    company_master_clean %>%
      select(ticker, size_bucket, sector, canton),
    by = c("symbol" = "ticker")
  ) %>%
  filter(!is.na(size_bucket), !is.na(sector), !is.na(canton))

# ✓ DATA QUALITY VERIFICATION
cat("✓ Monthly returns panel created\n")
cat("  - Time period:", format(min(monthly_returns$month), "%b %Y"), 
    "to", format(max(monthly_returns$month), "%b %Y"), "\n")
cat("  - Total monthly observations:", nrow(monthly_returns), "\n")
cat("  - Unique companies:", n_distinct(monthly_returns$symbol), "\n")
cat("  - Size bucket distribution:\n")
print(
  monthly_returns %>%
    group_by(size_bucket) %>%
    summarise(
      n_companies = n_distinct(symbol),
      n_observations = n(),
      mean_return = mean(ret_monthly, na.rm = TRUE),
      sd_return = sd(ret_monthly, na.rm = TRUE),
      .groups = "drop"
    )
)
cat("\n")

# Check for missing values
missing_summary <- monthly_returns %>%
  summarise(
    across(everything(), list(missing = ~ sum(is.na(.))),
           .names = "{.col}_missing")
  )

cat("✓ Missing value check:\n")
cat("  - Records with missing returns:", sum(is.na(monthly_returns$ret_monthly)), "\n")
cat("  - Records with missing size:", sum(is.na(monthly_returns$size_bucket)), "\n")
cat("  - Records with missing sector:", sum(is.na(monthly_returns$sector)), "\n")
cat("  - Records with missing canton:", sum(is.na(monthly_returns$canton)), "\n")

# Verify all three size categories present
size_categories <- monthly_returns %>% pull(size_bucket) %>% unique() %>% na.omit()
cat("✓ Size categories present:", paste(size_categories, collapse = ", "), "\n\n")

# ============================================================================
# SUMMARY OF FEATURE ENGINEERING
# ============================================================================
cat("\n=== FEATURE ENGINEERING COMPLETE ===\n")
cat("Total features:", ncol(company_master_clean), "\n")
cat("New features added:", ncol(company_master_clean) - ncol(company_master), "\n")

# Display first few rows with new features
cat("\nSample of engineered features:\n")
print(company_master_clean %>% 
        select(ticker, name, region, sector_broad, size_bucket, 
               is_zug, companies_in_canton) %>% 
        head(10))
```

**Interpretation:** Feature engineering creates derived variables that enable more sophisticated analysis:

1. **Geographic groupings**: Regions and financial center flags allow analysis beyond individual cantons
2. **Sector aggregation**: Broader categories reduce dimensionality while preserving economic meaning
3. **Size indicators**: Multiple representations enable both regression and comparison analyses
4. **Interaction terms**: Canton-sector combinations capture concentration effects
5. **Density metrics**: Company counts quantify clustering within geographic and sectoral dimensions

These features will be crucial for testing hypotheses about size premiums, geographic advantages, and concentration risk.


### Data Validation

```{r data-validation, echo=TRUE, fig.width=14, fig.height=6, fig.cap="Figure 2: Data Validation - Geographic Coverage"}
# ============================================================================
# VALIDATION CHECK 1: GEOGRAPHIC COVERAGE
# ============================================================================
cat("=== GEOGRAPHIC COVERAGE VALIDATION ===\n")

# Check how many companies have valid coordinates
geo_coverage <- company_master_clean %>%
  summarise(
    total_companies = n(),
    with_coordinates = sum(!is.na(latitude) & !is.na(longitude)),
    missing_coordinates = sum(is.na(latitude) | is.na(longitude)),
    coverage_pct = (with_coordinates / total_companies) * 100
  )

cat(sprintf("Total companies: %d\n", geo_coverage$total_companies))
cat(sprintf("With valid coordinates: %d (%.1f%%)\n", 
            geo_coverage$with_coordinates, 
            geo_coverage$coverage_pct))
cat(sprintf("Missing coordinates: %d\n", geo_coverage$missing_coordinates))

# ============================================================================
# VALIDATION CHECK 2: CANTON-SECTOR COMBINATIONS
# ============================================================================
cat("\n=== CANTON-SECTOR DISTRIBUTION ===\n")

# Create a heatmap of company counts by canton and sector
canton_sector_matrix <- company_master_clean %>%
  count(canton, sector) %>%
  pivot_wider(names_from = sector, values_from = n, values_fill = 0)

cat("Canton-sector combinations:\n")
cat(sprintf("  - Total cantons: %d\n", nrow(canton_sector_matrix)))
cat(sprintf("  - Total sectors: %d\n", ncol(canton_sector_matrix) - 1))
cat(sprintf("  - Possible combinations: %d\n", 
            nrow(canton_sector_matrix) * (ncol(canton_sector_matrix) - 1)))
cat(sprintf("  - Non-empty combinations: %d\n", 
            sum(canton_sector_matrix[, -1] > 0)))

cat("\n✓ Data validation complete\n")
```

**Interpretation:** 

- **Geographic coverage**: All 188 companies have valid Swiss coordinates, enabling spatial analysis
- **Canton-sector distribution**: Not all combinations exist (sparse matrix), which is expected as certain sectors naturally cluster in specific cantons
- **Data completeness**: High completeness across all variables ensures reliable downstream analysis

The validation confirms the dataset is ready for hypothesis testing and modeling.


# Chapter of Choice: Web Scraping (SPI Company List)

In this chapter, we focus exclusively on **web scraping**. This section documents how the list of SPI companies was collected, which represents the **first step** in building Dataset A.

## Motivation

There is no freely available dataset that provides a complete list of SPI constituents with the identifiers needed for this project. The official SIX API offers such data, but access is paid. We therefore collected the company list from a public source using web scraping.

Source: `https://www.finanzen.ch/index/liste/spi`

The website displays SPI constituents across multiple pages and does not provide a direct download option.

## Web Scraping Challenge

The SPI company list is distributed across **five pages** and requires page navigation. Since the content is rendered dynamically and depends on browser interaction, **classic static HTML scraping is not sufficient**.

We therefore used **browser-based scraping**.

## Technical Approach

We implemented the scraping process using:

- **`RSelenium`** to control a browser session and navigate pages
- **`rvest`** to parse the rendered HTML and extract the table
- **`tidyverse`** to clean and combine the data

The workflow:

1. Open the SPI webpage in a browser session
2. Extract the single table containing the company list
3. Loop through all five pages using automated clicks
4. Parse and store the table content for each page
5. Append all pages into one dataset

## Output of the Web Scraping Step

The web scraping step produces a raw list of SPI companies. The most important fields obtained directly from the website are:

- `Name` (Company name)
- `ISIN` (International Securities Identification Number)

The website also provides additional market snapshot columns (e.g., short-term return indicators). These fields are retained in the raw output but are not used directly in the empirical analysis.

The scraped data is stored locally to ensure reproducibility and to avoid repeated web scraping.

This scraping step represents **Stage 1 of Dataset A**. All subsequent enrichment steps (adding addresses, geographic coordinates, sector classification) are performed separately and merged with the scraped data to create the final company master dataset.



# Descriptive Analysis: Company Distribution

## Overview Statistics by Canton, Sector, and Size

In this section, we examine how Swiss companies are distributed across geographic and industry dimensions. Understanding this distribution is critical for assessing diversification opportunities and identifying potential concentration risks.

### Company distribution by canton {.tabset}

####

```{r descriptive-table-1, echo=TRUE}
# ============================================================================
# TABLE 1: COMPANY DISTRIBUTION BY CANTON
# ============================================================================
canton_summary <- company_master_clean %>%
  group_by(canton) %>%
  summarise(
    n_companies = n(),
    n_sectors = n_distinct(sector),
    n_small = sum(size_bucket == "Small"),
    n_mid = sum(size_bucket == "Mid"),
    n_large = sum(size_bucket == "Large"),
    pct_total = (n() / nrow(company_master_clean)) * 100
  ) %>%
  arrange(desc(n_companies))

# Create the table
tbl_canton <- kable(canton_summary, 
      digits = 1,
      col.names = c("Canton", "Companies", "Sectors", "Small", "Mid", "Large", "% of Total"),
      caption = "Table 2: Company Distribution by Canton",
      align = c("l", "r", "r", "r", "r", "r", "r")) %>%
  kable_styling(bootstrap_options = c("striped", "hover"),
                full_width = FALSE) %>%
  row_spec(1:3, bold = TRUE, background = "#f0f0f0")

# Display table
tbl_canton
```

####

```{r treemap-1, echo=TRUE}
# ============================================================================
# TREEMAP
# ============================================================================

# Create the treemap
p_canton_treemap <- canton_summary %>%
  ggplot(aes(area = n_companies, fill = n_companies, label = canton)) +
  geom_treemap(colour = "white", size = 2) +
  geom_treemap_text(
    colour = "white",
    place = "centre",
    size = 12,
    fontface = "bold",
    reflow = TRUE  # Automatically adjusts text size to fit
  ) +
  scale_fill_gradient(
    low = "#abd9e9",
    high = "#2c7bb6",
    name = "Companies"
  ) +
  labs(title = "Canton treemap by company count") +
  theme_minimal(base_size = 11) +
  theme(
    plot.title = element_text(face = "bold", size = 12),
    legend.position = "bottom"
  )

p_canton_treemap
```

**Interpretation:**

The canton-level distribution reveals significant geographic concentration:

- **Zürich dominates** with approximately 26% of all SPI companies, hosting firms across multiple sectors. This reflects Zürich's role as Switzerland's primary financial and commercial center.
- **Zug ranks second** despite being a small canton, suggesting high company density. This supports the client's hypothesis about Zug's attractiveness.
- **Top 5 cantons account for >60% of companies**, indicating that geographic diversification requires deliberate effort beyond selecting companies randomly.
- **Sector diversity varies**: Larger cantons (Zürich, Zug) host more diverse sector representation, while smaller cantons may be dominated by one or two industries.


### Company distribution by sector {.tabset}

####

```{r descriptive-table-2, echo=TRUE, warning=FALSE}
# ============================================================================
# TABLE 2: COMPANY DISTRIBUTION BY SECTOR
# ============================================================================
sector_summary <- company_master_clean %>%
  group_by(sector) %>%
  summarise(
    n_companies = n(),
    n_cantons = n_distinct(canton),
    n_small = sum(size_bucket == "Small"),
    n_mid = sum(size_bucket == "Mid"),
    n_large = sum(size_bucket == "Large"),
    pct_total = (n() / nrow(company_master_clean)) * 100,
    .groups = "drop"
  ) %>%
  arrange(desc(n_companies))

# Sector color palette
sector_colors <- c(
  "Industrial Goods" = "#E74C3C",
  "Health Care" = "#3498DB",
  "Banks" = "#F39C12",
  "Financial Services" = "#F1C40F",
  "Technology" = "#9B59B6",
  "Consumer Products" = "#1ABC9C",
  "Food & Beverage" = "#27AE60",
  "Chemicals" = "#E67E22",
  "Insurance" = "pink",
  "Real Estate" = "#34495E",
  "Telecommunications" = "#16A085",
  "Utilities" = "#7F8C8D"
)

# Create the table
tbl_sector <- sector_summary %>%
  kable(
    digits = 1,
    col.names = c("Sector", "Companies", "Cantons", "Small", "Mid", "Large", "% of Total"),
    caption = "Table 3: Company Distribution by Sector",
    align = c("l", "r", "r", "r", "r", "r", "r")) %>%
  kable_styling(bootstrap_options = c("striped", "hover"),
                full_width = FALSE) %>%
  row_spec(1:3, bold = TRUE, background = "#f0f0f0")

tbl_sector
```

####

```{r pareto-1, echo=TRUE, warning=FALSE}
# ============================================================================
# PARETO CHART
# ============================================================================
# Create the Pareto chart
pareto_data <- sector_summary %>%
  mutate(
    cumsum_companies = cumsum(n_companies),
    cumulative_pct = cumsum_companies / sum(n_companies) * 100,
    sector = factor(sector, levels = sector)  # Preserve order
  )

p_sector_pareto <- ggplot(pareto_data, aes(x = sector, y = n_companies)) +
  # Bar chart for company count
  geom_col(fill = sector_colors, alpha = 0.85, width = 0.7) +
  # Cumulative line (scaled to fit on same axis)
  geom_line(
    aes(y = cumulative_pct / 100 * max(n_companies), group = 1),
    colour = sector_colors,
    linewidth = 1.1,
    alpha = 0.8
  ) +
  geom_point(
    aes(y = cumulative_pct / 100 * max(pareto_data$n_companies)),
    colour = sector_colors,
    size = 2.5,
    alpha = 0.8
  ) +
  # Add cumulative percentage labels on the line
  geom_text(
    aes(
      y = cumulative_pct / 100 * max(pareto_data$n_companies),
      label = paste0(round(cumulative_pct, 0), "%")
    ),
    vjust = -1,
    size = 3,
    colour = sector_colors
  ) +
  # Secondary axis for percentage
  scale_y_continuous(
    name = "Number of companies",
    sec.axis = sec_axis(
      trans = ~ . / max(pareto_data$n_companies) * 100,
      name = "Cumulative %",
      labels = scales::percent_format(scale = 1)
    )
  ) +
  coord_flip() +
  labs(title = "Sector concentration (Pareto)") +
  theme_minimal(base_size = 11) +
  theme(
    plot.title = element_text(face = "bold", size = 12),
    axis.title.y = element_blank(),
    axis.text.y = element_text(size = 10),
    axis.title.x.right = element_text(colour = sector_colors, size = 10),
    axis.title.x.left = element_text(size = 10)
  )

p_sector_pareto
```

**Interpretation:**

Sector-level patterns reveal Switzerland's industrial strengths:

- **Industrial Goods dominates** with ~51% of all companies, reflecting Switzerland's manufacturing heritage (precision engineering, machinery, automation).
- **Health Care is second-largest**, driven by Switzerland's pharmaceutical giants and biotech innovation ecosystem.
- **Banks and Financial Services** combined represent significant portion, consistent with Switzerland's role as a global financial hub.
- **Size distribution within sectors varies**: Health Care and Banks have more large-cap representation, while Industrial Goods spans all size categories.

This sectoral concentration has implications for sector-based diversification strategies.


### Company distribution by size bucket {.tabset}

####

```{r descriptive-table-3, echo=TRUE}
# ============================================================================
# TABLE 3: COMPANY DISTRIBUTION BY SIZE BUCKET
# ============================================================================
size_summary <- company_master_clean %>%
  group_by(size_bucket) %>%
  summarise(
    n_companies = n(),
    n_cantons = n_distinct(canton),
    n_sectors = n_distinct(sector),
    pct_total = (n() / nrow(company_master_clean)) * 100,
    .groups = "drop"
  ) %>%
  arrange(match(size_bucket, c("Large", "Mid", "Small")))

# Create the table
tbl_size <- size_summary %>%
  kable(
    digits = 1,
    col.names = c("Size Bucket", "Companies", "Cantons", "Sectors", "% of Total"),
    caption = "Table 4: Company Distribution by Size Bucket",
    align = c("l", "r", "r", "r", "r")
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "full_width" = FALSE))

tbl_size
```

####

```{r radar-3, echo=TRUE}
# ============================================================================
# TABLE 3: COMPANY DISTRIBUTION BY SIZE BUCKET
# ============================================================================

# Prepare data for radar chart
# Normalize each metric to 0-100 scale for visual comparison
radar_data <- size_summary %>%
  mutate(
    # Normalize each metric (0-100 scale)
    companies_norm = (n_companies / max(n_companies)) * 100,
    cantons_norm   = (n_cantons / max(n_cantons)) * 100,
    sectors_norm   = (n_sectors / max(n_sectors)) * 100
  ) %>%
  select(size_bucket, companies_norm, cantons_norm, sectors_norm) %>%
  pivot_longer(
    cols = c(companies_norm, cantons_norm, sectors_norm),
    names_to = "metric",
    values_to = "value"
  ) %>%
  mutate(
    metric = factor(
      metric,
      levels = c("companies_norm", "cantons_norm", "sectors_norm"),
      labels = c("Companies", "Cantons", "Sectors")
    ),
    size_bucket = factor(
      size_bucket,
      levels = c("Large", "Mid", "Small")
    )
  )

# Create radar/spider chart using coord_polar
p_size_radar <- ggplot(radar_data, aes(x = metric, y = value, group = size_bucket, colour = size_bucket)) +
  # Background grid lines
  geom_hline(yintercept = seq(0, 100, 25), colour = "grey90", linewidth = 0.4) +
  # Polygon fill
  geom_polygon(aes(fill = size_bucket), alpha = 0.15, linewidth = 0) +
  # Lines
  geom_line(linewidth = 1.2, alpha = 0.8) +
  # Points at vertices
  geom_point(size = 3, alpha = 0.9) +
  # Convert to polar coordinates for radar
  coord_polar() +
  # Color scheme
  scale_colour_manual(
    name = NULL,
    values = c("Large" = "#d7191c", "Mid" = "#fdae61", "Small" = "#2c7bb6")
  ) +
  scale_fill_manual(
    name = NULL,
    values = c("Large" = "#d7191c", "Mid" = "#fdae61", "Small" = "#2c7bb6")
  ) +
  scale_y_continuous(
    limits = c(0, 100),
    breaks = seq(0, 100, 25),
    labels = NULL  # Hide radial labels for cleaner look
  ) +
  labs(
    title = "Size buckets: multi-dimensional profile",
    subtitle = "Normalized scores (0-100)"
  ) +
  theme_minimal(base_size = 11) +
  theme(
    plot.title = element_text(face = "bold", size = 12),
    plot.subtitle = element_text(size = 10, colour = "grey40"),
    legend.position = "bottom",
    legend.margin = margin(t = 8),
    axis.title = element_blank(),
    axis.text.x = element_text(size = 10, face = "bold"),
    axis.text.y = element_blank(),
    panel.grid.major.x = element_line(colour = "grey80", linewidth = 0.5),
    panel.grid.minor = element_blank()
  )
p_size_radar

```
**Interpretation:**

The size distribution reveals a typical market structure:

- **Small-caps dominate numerically** with 85% of companies. This is characteristic of broad market indices where large-caps dominate by market capitalization but small-caps dominate by count.
- **Large-caps are concentrated**: Only 13 large companies (7%), but these likely represent majority of total market capitalization.
- **Hidden champions hypothesis**: The abundance of small-caps supports the client's interest in discovering overlooked opportunities beyond well-known large firms.

This distribution motivates the **H1 hypothesis test** about whether small-caps offer higher returns as compensation for lower liquidity and analyst coverage.


## Advanced Concentration Analysis

*To assess whether canton-based investing exposes investors to hidden sector bets*, we measure return concentration using the Herfindahl-Hirschman Index (HHI).

> **Definition of Concentration Risk (H3):**
> We define *concentration risk* as the cross-sectional clustering of companies within canton–sector groups. Specifically, we measure the *sectoral HHI within each canton*: if a canton's companies are dominated by one sector, its HHI is high, meaning an investor diversifying "by canton" is actually making a sector bet.

This operationalization matters because: a portfolio spread across 5 different cantons may still have 80% exposure to one sector if those cantons are sector-concentrated.

```{r concentration-analysis, echo=TRUE, fig.width=14, fig.height=8, fig.cap="Figure 4: Concentration Risk Assessment"}
# ============================================================================
# HERFINDAHL-HIRSCHMAN INDEX (HHI) CALCULATION
# ============================================================================
# HHI measures market concentration; higher values = more concentrated

# HHI by canton (sector concentration within each canton)
canton_hhi <- company_master_clean %>%
  group_by(canton) %>%
  summarise(
    n_companies = n(),
    n_sectors = n_distinct(sector),
    # HHI = sum of squared market shares
    hhi = {
      shares <- table(sector) / n()
      sum(shares^2) * 10000  # Scale to 0-10000
    },
    .groups = "drop"
  ) %>%
  mutate(
    concentration_level = case_when(
      hhi >= 2500 ~ "High (HHI >= 2500)",
      hhi >= 1500 ~ "Moderate (1500-2499)",
      TRUE ~ "Low (HHI < 1500)"
    )
  ) %>%
  arrange(desc(hhi))

# Display top/bottom 10
cat("=== TOP 10 MOST CONCENTRATED CANTONS (by sector) ===\n")
print(canton_hhi %>% head(10))

cat("\n=== TOP 10 MOST DIVERSIFIED CANTONS (by sector) ===\n")
print(canton_hhi %>% tail(10))

# Visualization: HHI by canton
p_hhi <- ggplot(canton_hhi %>% filter(n_companies >= 5),  # Only cantons with 5+ companies
                aes(x = reorder(canton, hhi), y = hhi, fill = concentration_level)) +
  geom_col(alpha = 0.85) +
  geom_hline(yintercept = 2500, linetype = "dashed", color = "darkred", linewidth = 1) +
  geom_hline(yintercept = 1500, linetype = "dashed", color = "orange", linewidth = 1) +
  geom_text(aes(label = round(hhi, 0)), hjust = -0.2, size = 3) +
  coord_flip() +
  scale_fill_manual(
    values = c("High (HHI >= 2500)" = "#d7191c",
              "Moderate (1500-2499)" = "#fdae61",
              "Low (HHI < 1500)" = "#1a9850"),
    name = "Concentration Level"
  ) +
  annotate("text", x = 2, y = 2600, label = "High concentration\n(single-sector risk)",
           hjust = 0, size = 3, color = "darkred") +
  theme_minimal(base_size = 11) +
  labs(title = "Sectoral Concentration within Cantons (HHI Index)",
       subtitle = "Higher HHI = company returns more exposed to single sector risk",
       x = NULL, y = "HHI Score (0-10,000)",
       caption = "Note: Only cantons with 5+ companies shown. HHI >= 2500 indicates high concentration.") +
  theme(plot.title = element_text(face = "bold"),
        legend.position = "right",
        panel.grid.major.y = element_blank())

print(p_hhi)

# ============================================================================
# EXPLICIT CLUSTER RISK INTERPRETATION FOR INVESTOR
# ============================================================================
cat("\n=== INTERPRETATION FOR THE INVESTOR ===\n")
cat("A high HHI means that investing in this canton is effectively a BET on a single sector.\n")
cat("\nExample: Basel-Stadt (if HHI > 5000)\n")
cat("  -> High exposure to Healthcare/Pharma.\n")
cat("  -> If the Pharma sector falls, the entire cantonal portfolio falls.\n")
cat("  -> An investor who 'diversifies' into 5 Basel companies still has ~80% Pharma exposure.\n")
cat("\nIMPLICATION: Geographic spread != diversification. Must actively manage sector weights.\n")
```

**Interpretation:**

The Herfindahl-Hirschman Index (HHI) quantifies concentration risk:

- **High HHI cantons (>=2500)**: Dominated by one or two sectors. Investing in multiple companies from these cantons provides limited sector diversification. Example: If a canton has HHI > 5000, it's essentially a single-sector canton.

- **Moderate HHI cantons (1500-2499)**: Some sectoral diversity but still concentrated. Requires careful sector balance when constructing portfolios.

- **Low HHI cantons (<1500)**: Well-diversified across sectors. These cantons naturally provide better diversification for region-focused strategies.

> **Investor Takeaway:** A high HHI canton means your "geographic diversification" is illusory. An investor who only invests in Basel-based companies is implicitly making an 80%+ bet on the pharmaceutical sector. If Pharma stocks decline, the entire "cantonal portfolio" declines in lockstep.

This analysis directly relates to **H3 (Concentration Risk Hypothesis)**: Canton selection alone is insufficient for diversification—must explicitly manage sector exposure.


# Performance Analysis: Returns by Size

*Why this step matters for the investor:* The client believes that small-cap "hidden champions" outperform large-caps. However, an institutional investor (like a US Pension Fund) never evaluates returns in isolation—they always consider **return per unit of risk**. A small-cap that returns 2% more per month is worthless if its volatility is twice as high. This section provides the risk-adjusted evidence needed to test H1 properly.

## Descriptive Statistics: Monthly Returns by Size Bucket

*(Note: This section requires Dataset B - monthly returns panel. The following code assumes you have run the data pipeline and generated `spi_monthly_panel_analysis.csv`)*

```{r load-returns-data, echo=TRUE, eval=FALSE}
# ============================================================================
# LOAD MONTHLY RETURNS PANEL
# ============================================================================
# This assumes you've run the Dataset B pipeline code from your original report

# Load the analysis-ready monthly panel
monthly_returns <- read_csv("data/spi_monthly_panel_analysis.csv")

# Load firm summary (for 3-year metrics)
firm_summary <- read_csv("data/spi_firm_summary_strict.csv")

# Load size indices (equal-weight portfolios)
size_indices <- read_csv("data/spi_size_indices.csv")

cat("✓ Returns data loaded successfully\n")
cat("Monthly observations:", nrow(monthly_returns), "\n")
cat("Unique companies:", n_distinct(monthly_returns$symbol), "\n")
cat("Date range:", min(monthly_returns$month), "to", max(monthly_returns$month), "\n")
```

```{r returns-descriptive-stats, echo=TRUE, eval=FALSE}
# ============================================================================
# DESCRIPTIVE STATISTICS TABLE: RETURNS BY SIZE (WITH SHARPE RATIO)
# ============================================================================
# CRITICAL FOR H1: We must evaluate risk-adjusted returns, not just raw returns.
# An investor cares about return per unit of risk (Sharpe Ratio).

# Calculate summary statistics by size bucket
returns_summary <- monthly_returns %>%
  filter(!is.na(size_bucket), !is.na(ret_monthly)) %>%
  group_by(size_bucket) %>%
  summarise(
    n_observations = n(),
    n_companies = n_distinct(symbol),
    mean_return = mean(ret_monthly, na.rm = TRUE),
    median_return = median(ret_monthly, na.rm = TRUE),
    sd_return = sd(ret_monthly, na.rm = TRUE),              # The Risk
    sharpe_ratio = mean(ret_monthly, na.rm = TRUE) / sd(ret_monthly, na.rm = TRUE),  # KEY METRIC
    min_return = min(ret_monthly, na.rm = TRUE),
    max_return = max(ret_monthly, na.rm = TRUE),
    q25_return = quantile(ret_monthly, 0.25, na.rm = TRUE),
    q75_return = quantile(ret_monthly, 0.75, na.rm = TRUE),
    skewness = moments::skewness(ret_monthly, na.rm = TRUE),
    kurtosis = moments::kurtosis(ret_monthly, na.rm = TRUE),
    .groups = "drop"
  )

# Display formatted table with Sharpe Ratio highlighted
kable(returns_summary,
      digits = 4,
      col.names = c("Size", "Obs", "Companies", "Mean", "Median", "SD", 
                   "Sharpe", "Min", "Max", "Q25", "Q75", "Skew", "Kurt"),
      caption = "Table 5: Monthly Return Statistics by Size Bucket (with Sharpe Ratio)",
      align = c("l", rep("r", 12))) %>%
  kable_styling(bootstrap_options = c("striped", "hover"),
                full_width = FALSE) %>%
  add_header_above(c(" " = 3, "Central Tendency" = 2, "Risk-Adjusted" = 2, "Dispersion" = 4, "Shape" = 2))

# Print key insight for the investor
cat("\n=== KEY INSIGHT FOR INVESTOR ===\n")
cat("Compare SHARPE RATIO across size buckets:\n")
cat("If Small-cap Sharpe > Large-cap Sharpe: Small-caps offer better risk-adjusted returns\n")
cat("If Small-cap Sharpe ≈ Large-cap Sharpe: No risk-adjusted premium for small-caps\n")
cat(sprintf("\nSmall-cap Sharpe: %.4f | Large-cap Sharpe: %.4f\n",
            returns_summary$sharpe_ratio[returns_summary$size_bucket == "Small"],
            returns_summary$sharpe_ratio[returns_summary$size_bucket == "Large"]))
```

**Interpretation:** *(To be filled after running with actual data)*

Key metrics to evaluate:

- **Sharpe Ratio** (THE decisive metric for H1): Higher Sharpe = better risk-adjusted returns. If Small-cap Sharpe > Large-cap Sharpe, then small-caps genuinely offer value. If roughly equal, the higher raw return is "paid for" by higher volatility.
- **Mean vs Median**: If mean > median, distribution is right-skewed (outliers pull average up)
- **Standard Deviation**: Higher SD in small-caps indicates greater return volatility
- **Min/Max range**: Shows potential extreme outcomes
- **Skewness**: Positive = more extreme positive returns; negative = more negative outliers
- **Kurtosis**: > 3 = fat tails (more extreme events than normal distribution predicts)

## Return Distribution Visualizations

```{r returns-distributions, echo=TRUE, eval=FALSE, fig.width=16, fig.height=10, fig.cap="Figure 5: Return Distribution Comparison by Size Bucket"}
# ============================================================================
# MULTI-PANEL RETURN DISTRIBUTION VISUALIZATION
# ============================================================================

# Prepare data: filter to size buckets only (exclude missing)
returns_viz_data <- monthly_returns %>%
  filter(!is.na(size_bucket), !is.na(ret_monthly)) %>%
  mutate(size_bucket = factor(size_bucket, levels = c("Large", "Mid", "Small")))

# Panel A: Box plot with jittered points
p_boxplot <- ggplot(returns_viz_data, 
                    aes(x = size_bucket, y = ret_monthly, fill = size_bucket)) +
  geom_boxplot(alpha = 0.7, outlier.shape = NA) +
  geom_jitter(alpha = 0.05, width = 0.2, size = 0.5, color = "black") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red", alpha = 0.5) +
  scale_fill_manual(values = c("Large" = "#d7191c", "Mid" = "#fdae61", "Small" = "#2c7bb6")) +
  scale_y_continuous(labels = scales::percent_format()) +
  theme_minimal(base_size = 11) +
  labs(title = "A) Box Plot with Individual Observations",
       x = "Size Bucket", y = "Monthly Return") +
  theme(plot.title = element_text(face = "bold"),
        legend.position = "none",
        panel.grid.major.x = element_blank())

# Panel B: Violin plot (density distribution)
p_violin <- ggplot(returns_viz_data, 
                   aes(x = size_bucket, y = ret_monthly, fill = size_bucket)) +
  geom_violin(alpha = 0.6, trim = FALSE) +
  geom_boxplot(width = 0.15, alpha = 0.8, outlier.shape = NA) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red", alpha = 0.5) +
  scale_fill_manual(values = c("Large" = "#d7191c", "Mid" = "#fdae61", "Small" = "#2c7bb6")) +
  scale_y_continuous(labels = scales::percent_format()) +
  theme_minimal(base_size = 11) +
  labs(title = "B) Violin Plot (Distribution Shape)",
       x = "Size Bucket", y = "Monthly Return") +
  theme(plot.title = element_text(face = "bold"),
        legend.position = "none",
        panel.grid.major.x = element_blank())

# Panel C: Density ridges (overlapping distributions)
library(ggridges)
p_ridge <- ggplot(returns_viz_data, 
                  aes(x = ret_monthly, y = size_bucket, fill = size_bucket)) +
  geom_density_ridges(alpha = 0.7, scale = 2) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "red", alpha = 0.5) +
  scale_fill_manual(values = c("Large" = "#d7191c", "Mid" = "#fdae61", "Small" = "#2c7bb6")) +
  scale_x_continuous(labels = scales::percent_format(), limits = c(-0.3, 0.3)) +
  theme_minimal(base_size = 11) +
  labs(title = "C) Ridgeline Plot (Overlapping Densities)",
       x = "Monthly Return", y = "Size Bucket") +
  theme(plot.title = element_text(face = "bold"),
        legend.position = "none")

# Panel D: Diverging Bar Chart (Sector Performance vs Market Average)
# Calculate sector performance deviation from market mean
sector_deviation <- returns_viz_data %>%
  group_by(sector) %>%
  summarise(mean_return = mean(ret_monthly, na.rm = TRUE), .groups = "drop") %>%
  filter(!is.na(sector)) %>%
  mutate(
    market_avg = mean(mean_return),
    deviation = mean_return - market_avg,
    performance = ifelse(deviation > 0, "Above Average", "Below Average")
  ) %>%
  arrange(deviation)

p_diverging <- ggplot(sector_deviation, 
                 aes(x = reorder(sector, deviation), y = deviation, fill = performance)) +
  geom_col(alpha = 0.8, width = 0.7) +
  geom_hline(yintercept = 0, linewidth = 1, color = "black") +
  coord_flip() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 0.01)) +
  scale_fill_manual(values = c("Below Average" = "#d7191c", "Above Average" = "#1a9850")) +
  theme_minimal(base_size = 11) +
  labs(title = "D) Sector Performance vs Market Average",
       subtitle = "Diverging bar: deviation from SPI monthly mean",
       x = NULL, y = "Deviation from Market Average") +
  theme(plot.title = element_text(face = "bold"),
        legend.position = "none",
        panel.grid.major.y = element_blank())

# Combine panels
(p_boxplot + p_violin) / (p_ridge + p_diverging) +
  plot_annotation(
    title = "Monthly Return Distributions by Size Bucket",
    subtitle = "Multiple complementary visualizations reveal distribution characteristics",
    theme = theme(plot.title = element_text(size = 16, face = "bold"))
  )

```

**Interpretation:** *(To be completed with actual data)*

What to look for in these visualizations:
- **Box plot**: Shows median, quartiles, and outliers. Longer boxes = more variable returns.
- **Violin plot**: Reveals distribution shape. Width at any point = density of observations.
- **Ridgeline plot**: Directly compares distribution shapes. Easier to see if distributions overlap or are distinct.
- **Diverging bar chart**: Shows which sectors outperform/underperform the market average. Green bars = above average, red = below.

If H1 is true (small-caps have higher returns), we expect:
- Small-cap distribution shifted to the right (higher returns)
- BUT also wider distribution (higher risk)
- Question: Does higher mean return compensate for higher volatility?

## Performance Over Time

```{r performance-dynamics, echo=TRUE, eval=FALSE, fig.width=14, fig.height=10, fig.cap="Figure 6: Performance Dynamics by Size Bucket"}
# ============================================================================
# CUMULATIVE PERFORMANCE AND ROLLING VOLATILITY
# ============================================================================

# Panel A: Cumulative returns (equal-weighted portfolios)
p_cumulative <- ggplot(size_indices, 
                      aes(x = month, y = cum_ret_ew, color = size_bucket, linewidth = size_bucket)) +
  geom_line(alpha = 0.9) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray50", alpha = 0.5) +
  scale_color_manual(
    name = "Size Bucket",
    values = c("Large" = "#d7191c", "Mid" = "#fdae61", "Small" = "#2c7bb6")
  ) +
  scale_linewidth_manual(
    name = "Size Bucket",
    values = c("Large" = 1.2, "Mid" = 1, "Small" = 0.8)
  ) +
  scale_y_continuous(labels = scales::percent_format()) +
  theme_minimal(base_size = 11) +
  labs(title = "A) Cumulative Returns (Equal-Weighted Portfolios)",
       subtitle = "Growth of $1 invested at start of period",
       x = "Month", y = "Cumulative Return") +
  theme(plot.title = element_text(face = "bold"),
        legend.position = "bottom")

# Panel B: Rolling 12-month volatility
rolling_vol <- monthly_returns %>%
  filter(!is.na(size_bucket), !is.na(ret_monthly)) %>%
  group_by(size_bucket, month) %>%
  summarise(avg_return = mean(ret_monthly, na.rm = TRUE), .groups = "drop") %>%
  group_by(size_bucket) %>%
  arrange(month) %>%
  mutate(
    # Calculate rolling 12-month standard deviation (annualized)
    rolling_vol_12m = rollapply(
      avg_return, 
      width = 12, 
      FUN = function(x) sd(x, na.rm = TRUE) * sqrt(12),
      fill = NA,
      align = "right"
    )
  ) %>%
  ungroup()

p_vol <- ggplot(rolling_vol, 
                aes(x = month, y = rolling_vol_12m, color = size_bucket)) +
  geom_line(linewidth = 1, alpha = 0.9) +
  scale_color_manual(
    name = "Size Bucket",
    values = c("Large" = "#d7191c", "Mid" = "#fdae61", "Small" = "#2c7bb6")
  ) +
  scale_y_continuous(labels = scales::percent_format()) +
  theme_minimal(base_size = 11) +
  labs(title = "B) Rolling 12-Month Volatility (Annualized)",
       subtitle = "Time-varying risk by size bucket",
       x = "Month", y = "Volatility (Standard Deviation)") +
  theme(plot.title = element_text(face = "bold"),
        legend.position = "bottom")

# Combine panels
p_cumulative / p_vol +
  plot_annotation(
    title = "Performance Dynamics: Returns and Risk Over Time",
    theme = theme(plot.title = element_text(size = 16, face = "bold"))
  )
```

**Interpretation:** *(To be completed with actual data)*

Key observations to make:
- **Cumulative returns**: Do small-caps outperform over full period? Or is there period dependence (small-caps lead in some years, lag in others)?
- **Volatility spikes**: All size buckets should show volatility spikes during market crises (e.g., COVID-19 in early 2020). Do small-caps have consistently higher volatility?
- **Recovery patterns**: After market corrections, which size bucket recovers faster?

If H1 is true, we expect:
- Small-cap cumulative line above large-cap at end of period
- BUT small-cap volatility consistently higher
- Risk-adjusted comparison (Sharpe ratio) needed to determine if excess return justifies excess risk

### Cumulative Returns: Area Chart Visualization

*This area chart provides an alternative view of portfolio growth, showing the relative magnitude of cumulative returns across size buckets.*

```{r area-chart-cumulative, echo=TRUE, eval=FALSE, fig.width=12, fig.height=6, fig.cap="Figure: Cumulative Returns by Size Bucket (Area Chart)"}
# ============================================================================
# AREA CHART: CUMULATIVE RETURNS BY SIZE BUCKET
# ============================================================================

# Calculate equal-weighted portfolio returns by size bucket
portfolio_returns <- monthly_returns %>%
  filter(!is.na(size_bucket), !is.na(ret_monthly)) %>%
  group_by(month, size_bucket) %>%
  summarise(bucket_return = mean(ret_monthly, na.rm = TRUE), .groups = "drop") %>%
  arrange(size_bucket, month) %>%
  group_by(size_bucket) %>%
  mutate(cumulative = cumprod(1 + bucket_return) - 1) %>%
  ungroup() %>%
  mutate(size_bucket = factor(size_bucket, levels = c("Large", "Mid", "Small")))

ggplot(portfolio_returns, aes(x = month, y = cumulative, fill = size_bucket)) +
  geom_area(alpha = 0.5, position = "identity") +
  geom_line(aes(color = size_bucket), linewidth = 0.8, alpha = 0.8) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray30") +
  scale_y_continuous(labels = scales::percent_format()) +
  scale_fill_manual(
    name = "Size Bucket",
    values = c("Large" = "#d7191c", "Mid" = "#fdae61", "Small" = "#2c7bb6")
  ) +
  scale_color_manual(
    name = "Size Bucket",
    values = c("Large" = "#d7191c", "Mid" = "#fdae61", "Small" = "#2c7bb6")
  ) +
  labs(
    title = "Cumulative Returns by Size Bucket (Equal-Weighted Portfolios)",
    subtitle = "Overlapping areas show relative performance | Line edges mark exact cumulative values",
    x = NULL,
    y = "Cumulative Return"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    legend.position = "bottom",
    panel.grid.minor = element_blank()
  )
```

**Interpretation:**

The area chart provides a visually intuitive way to compare portfolio growth:
- **Larger areas**: Size buckets with higher cumulative returns over time
- **Area overlap**: Shows where different size buckets have similar performance
- **Area separation**: Reveals divergences in performance between size categories


# Geospatial Analysis: Economic Maps of Switzerland

*Why this step matters for the investor:* Geography shapes investor exposure in ways that spreadsheets obscure. Understanding where companies cluster—and which sectors dominate which cantons—helps the client avoid unintentional concentration bets. A "diversified" list of 10 companies from one canton may be a de facto single-sector portfolio.

## Map 1: Company Count by Canton (Bubble Map)

*(Note: This section uses the geospatial code from your original report. I'm including it with enhanced interpretations.)*

```{r prepare-map1-data, message=FALSE, warning=FALSE, echo=TRUE}
# ============================================================================
# PREPARE GEOSPATIAL DATA FOR MAPPING
# ============================================================================

# Read Swiss canton shapefile and normalize using previously defined function
layer_cantons <- st_read("./files_maps_CH/G1K09.shp",
                         options = "ENCODING=UTF-8",
                         quiet = TRUE) %>%
  st_transform(4326) %>%
  mutate(
    NAME = enc2utf8(NAME),
    canton_key = normalize_canton_names(NAME),
    canton = factor(str_to_title(canton_key), levels = sort(unique(
      str_to_title(canton_key)
    )))
  )

# Create point geometries for company locations
company_locations <- company_geo %>%
  filter(!is.na(longitude), !is.na(latitude)) %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326)

# Canton summary: company counts and dominant sector
canton_summary_geo <- company_geo %>%
  filter(!is.na(canton_key), canton_key != "") %>%
  group_by(canton_key) %>%
  summarise(
    n_companies = n(),
    n_sectors = n_distinct(sector),
    n_large = sum(size_bucket == "Large", na.rm = TRUE),
    n_mid = sum(size_bucket == "Mid", na.rm = TRUE),
    n_small = sum(size_bucket == "Small", na.rm = TRUE),
    # Dominant sector (most common)
    dominant_sector = names(sort(table(sector), decreasing = TRUE))[1],
    lat = mean(latitude, na.rm = TRUE),
    lon = mean(longitude, na.rm = TRUE),
    .groups = "drop"
  )

# Join canton counts onto polygons
swiss_map_data <- layer_cantons %>%
  left_join(canton_summary_geo, by = "canton_key") %>%
  mutate(n_companies = replace_na(n_companies, 0))

# Calculate bubble positions
canton_centroids <- st_centroid(swiss_map_data) %>%
  st_coordinates() %>%
  as.data.frame() %>%
  rename(centroid_lon = X, centroid_lat = Y)

canton_bubbles <- bind_cols(
  swiss_map_data %>% st_drop_geometry(),
  canton_centroids
) %>%
  mutate(
    bubble_lon = ifelse(!is.na(lon), lon, centroid_lon),
    bubble_lat = ifelse(!is.na(lat), lat, centroid_lat)
  ) %>%
  filter(!is.na(n_companies), n_companies > 0)

cat("Geospatial data prepared\n")
cat("  - Cantons with companies:", nrow(canton_bubbles), "\n")
cat("  - Company locations mapped:", nrow(company_locations), "\n")

```

```{r map-1-bubbles, fig.cap="Figure 7: The Economic Map of Switzerland - Company Count by Canton", fig.width=14, fig.height=10}
# ============================================================================
# MAP 1: BUBBLE MAP (COMPANY COUNT BY CANTON)
# ============================================================================

economic_map <- ggplot() +
  # Canton polygons (base layer)
  geom_sf(
    data = swiss_map_data,
    fill = "#E8E8E8",
    color = "#666666",
    linewidth = 0.8
  ) +
  # Bubbles: size = company count, color = dominant sector
  geom_point(
    data = canton_bubbles,
    aes(x = bubble_lon, y = bubble_lat,
        size = n_companies,
        fill = dominant_sector),
    alpha = 0.75,
    shape = 21,       # Circle with border
    color = "white",
    stroke = 0.8
  ) +
  # Size scale
  scale_size_continuous(
    name = "Number of Companies",
    range = c(6, 30),
    breaks = c(5, 10, 20, 50),
    labels = c("5", "10", "20", "50+")
  ) +
  # Fill (sector color) scale
  scale_fill_manual(
    name = "Dominant Sector",
    values = sector_colors,
    na.value = "gray70"
  ) +
  # Canton labels
  geom_label(
    data = canton_bubbles %>% filter(n_companies >= 10),
    aes(x = bubble_lon, y = bubble_lat, label = toupper(canton_key)),
    size = 3.5,
    fontface = "bold",
    fill = "white",
    linewidth = 0,     # no border
    alpha = 0.75
  ) + 
  geom_label(
    data = canton_bubbles %>% filter(n_companies < 10),
    aes(x = bubble_lon, y = bubble_lat, label = toupper(canton_key)),
    size = 3.5,
    fontface = "bold",
    fill = "white",
    linewidth = 0,     # no border
    alpha = 0.75,
    nudge_y = -0.07
  ) +
  theme_void(base_size = 12) +
  theme(
    plot.title = element_text(
      size = 18, face = "bold", hjust = 0.5,
      margin = margin(b = 8)
    ),
    plot.subtitle = element_text(
      size = 12, hjust = 0.5,
      margin = margin(b = 20), color = "gray40"
    ),
    legend.position = "bottom",
    legend.box = "vertical",
    legend.margin = margin(t = 10),
    legend.title = element_text(size = 11, face = "bold"),
    legend.text = element_text(size = 10),
    plot.margin = margin(15, 15, 10, 15),
    panel.background = element_rect(fill = "white", color = NA),
    plot.background = element_rect(fill = "white", color = NA),
    plot.caption = element_text(
      size = 9, hjust = 0, color = "gray50",
      margin = margin(t = 12)
    )
  ) +
  labs(
    title = "THE ECONOMIC MAP OF SWITZERLAND",
    subtitle = "SPI Listed Companies by Canton | Size = Company Count | Color = Dominant Sector",
    caption = sprintf(
      "Data: %d companies across %d cantons | Top canton: %s (%d firms)",
      sum(canton_bubbles$n_companies, na.rm = TRUE),
      nrow(canton_bubbles),
      canton_bubbles$canton_key[which.max(canton_bubbles$n_companies)],
      max(canton_bubbles$n_companies, na.rm = TRUE)
    )
  ) +
  guides(
    size = guide_legend(order = 1, override.aes = list(fill = "gray80", alpha = 0.75)),
    fill = guide_legend(order = 2, override.aes = list(size = 5, alpha = 0.75))
  )

print(economic_map)

```

**Interpretation:**

This bubble map reveals the spatial distribution of Swiss economic activity:

**Geographic Patterns:**
- **Zürich cluster**: Largest bubble indicates Switzerland's primary financial and commercial center
- **Arc lémanique (Lake Geneva)**: Genève and Vaud form a secondary cluster in western Switzerland
- **Basel region**: Concentrated in northwestern corner, strong pharmaceutical presence (blue bubbles)
- **Central Switzerland**: Zug punches above its size, likely due to favorable tax environment

**Sectoral Specialization:**
- **Color clustering**: Different regions have distinct sectoral characters
  - Red (Industrial Goods): Dispersed across central and eastern cantons
  - Blue (Health Care): Concentrated in Basel region (pharmaceutical hub)
  - Gold/Yellow (Financials): Zürich, Zug, Genève (banking centers)

**Implications for Portfolio Construction:**
- **Geographic diversification ≠ sector diversification**: Selecting companies from multiple cantons may still result in sector concentration
- **Hidden champion search**: Medium-sized bubbles in non-financial-center cantons may house undiscovered opportunities
- **Regional economic shocks**: Companies in the same canton may be exposed to common regional factors (cantonal policy, local labor markets, infrastructure)

This map provides visual evidence for **H3 (Concentration Risk)**: Geographic spread does not guarantee diversified exposure.


## Map 2: Dominant Sector by Trading Volume

*(Note: This map shows which sector has the highest cumulative trading volume in each canton)*

```{r prepare-map2-data, echo=TRUE}
# ============================================================================
# PREPARE DATA FOR REPRESENTATION OF SECTOR VOLUME BY CANTON
# ============================================================================

# Join the previously engineered features onto canton polygons
swiss_map_sectorvol <- layer_cantons %>%
  left_join(canton_top_sector_vol, by = "canton")

# Standardize accents in both datasets before any factor conversion

canton_top_sector_vol <- canton_top_sector_vol %>%
  mutate(
    canton = stri_trans_general(as.character(canton), "Any-Latin; Latin-ASCII")
  ) %>%
  mutate(canton = str_to_title(canton)) 

layer_cantons <- layer_cantons %>%
  mutate(
    canton = stri_trans_general(as.character(canton), "Any-Latin; Latin-ASCII")
  ) %>%
  mutate(canton = str_to_title(canton))  

# Now join
swiss_map_sectorvol <- layer_cantons %>%
  left_join(canton_top_sector_vol, by = "canton")

# Check for unmatched cantons from volume data
unmatched_volume_cantons <- canton_top_sector_vol %>%
  anti_join(layer_cantons %>% st_drop_geometry(), by = "canton") %>%
  pull(canton)

if (length(unmatched_volume_cantons) > 0) {
  cat(" Cantons in volume data NOT matched to shapefile:\n")
  cat("    ", paste(unmatched_volume_cantons, collapse = ", "), "\n")
} else {
  cat(" All canton volume data successfully matched to map!\n")
}

# Check for cantons in shapefile with no volume data
cantons_no_volume <- swiss_map_sectorvol %>%
  st_drop_geometry() %>%
  filter(is.na(top_sector_vol)) %>%
  pull(canton)

if (length(cantons_no_volume) > 0) {
  cat(" Cantons with no trading volume data (no SPI companies):", length(cantons_no_volume), "\n")
  cat("    ", paste(cantons_no_volume, collapse = ", "), "\n")
}

```

```{r map-2-sector-volume, fig.cap="Figure 8: Dominant Sector by Canton (Trading Volume)", fig.width=14, fig.height=10}
# ============================================================================
# MAP 2: CHOROPLETH MAP (SECTOR DOMINANCE BY VOLUME)
# ============================================================================

map_sectorvol <- ggplot() +
  # Canton fill = dominant sector by volume
  geom_sf(
    data = swiss_map_sectorvol,
    aes(fill = top_sector_vol),
    color = "white",
    linewidth = 0.7
  ) +
  # Overlay company locations as points (shape = size bucket)
  geom_sf(
    data = company_locations,
    aes(shape = size_bucket, color = size_bucket),
    alpha = 0.55,
    size = 2.5,
    stroke = 0.3
  ) +
  scale_fill_manual(
    name = "Top Sector (by Volume/Count)",
    values = sector_colors,
    na.value = "#f0f0f0"
  ) +
  scale_shape_manual(
    name = "Company Size",
    values = c("Small" = 17, "Mid" = 15, "Large" = 16)
  ) +
  scale_color_manual(
    name = "Company Size",
    values = c("Small" = "white", "Large" = "black"),
  ) +
  theme_void(base_size = 12) +
  theme(
    plot.title = element_text(
      size = 18, face = "bold", hjust = 0.5,
      margin = margin(b = 8)
    ),
    plot.subtitle = element_text(
      size = 12, hjust = 0.5,
      margin = margin(b = 20), color = "gray40"
    ),
    legend.position = "right",
    legend.title = element_text(face = "bold", size = 11),
    legend.text = element_text(size = 10),
    plot.margin = margin(15, 15, 10, 15),
    panel.background = element_rect(fill = "white", color = NA),
    plot.background = element_rect(fill = "white", color = NA),
    plot.caption = element_text(
      size = 9, hjust = 0, color = "gray50",
      margin = margin(t = 12)
    )
  ) +
  labs(
    title = "SWISS CANTONS: SECTOR DOMINANCE BY TRADING VOLUME",
    subtitle = "Canton fill = top sector (cumulative volume) | Points = company HQ (shape by size)",
    caption = sprintf(
      "Data: %d companies | %d cantons shown | Volume proxy: %s",
      nrow(company_locations),
      nrow(swiss_map_sectorvol),
      ifelse(exists("vol_col"), "Actual trading volume", "Company count")
    )
  ) +
  guides(
    fill = guide_legend(order = 1, ncol = 2),
    shape = guide_legend(order = 2),
    color = "none"  # Hide color legend (redundant with shape)
  )

print(map_sectorvol)
```

**Interpretation:**

This choropleth map reveals sectoral dominance patterns:

**Canton-Level Patterns:**
- **Basel region (blue)**: Pharmaceutical/Health Care dominance visible via canton fill color
- **Zürich & Zug (yellow/gold)**: Financial sector dominance
- **Central/Eastern Switzerland (red)**: Industrial Goods stronghold

**Company-Level Overlay:**
- **Point symbols show individual firm locations**: Shape indicates size bucket
- **Large-cap concentration**: Large firms (diamonds) cluster in major urban cantons
- **Small-cap dispersion**: Small firms (triangles) spread more evenly across regions

**Strategic Implications:**
- **Sector-based strategies must consider geography**: If targeting Health Care, Basel region is natural focus
- **Cantonal exposure = implicit sector bet**: Investing heavily in Basel companies likely overweights pharmaceuticals
- **Diversification requires deliberate sector balance**: Can't rely on geographic spread alone

This map provides additional evidence for **H2 and H3**: Canton characteristics (like Zug's tax policy) interact with sector composition, and concentration risk manifests at the canton-sector intersection.


### Canton x Sector Return Heatmap

*This heatmap provides a matrix view of mean returns across canton-sector combinations, highlighting interaction effects that may not be visible in univariate analyses.*

```{r heatmap-canton-sector, echo=TRUE, eval=FALSE, fig.width=14, fig.height=10, fig.cap="Figure: Canton x Sector Mean Returns Heatmap"}
# ============================================================================
# HEATMAP: CANTON × SECTOR MEAN RETURNS
# ============================================================================

# Calculate mean return by canton and sector
heatmap_data <- monthly_returns %>%
  filter(!is.na(canton), !is.na(sector), !is.na(ret_monthly)) %>%
  group_by(canton, sector) %>%
  summarise(
    mean_return = mean(ret_monthly, na.rm = TRUE),
    n_obs = n(),
    .groups = "drop"
  ) %>%
  filter(n_obs >= 12)  # At least 12 months of data for reliability

# Create heatmap
ggplot(heatmap_data, aes(x = sector, y = canton, fill = mean_return)) +
  geom_tile(color = "white", linewidth = 0.5) +
  geom_text(aes(label = scales::percent(mean_return, accuracy = 0.1)), 
            size = 2.5, color = ifelse(abs(heatmap_data$mean_return) > 0.01, "white", "black")) +
  scale_fill_gradient2(
    low = "#d7191c", 
    mid = "#ffffbf", 
    high = "#1a9850",
    midpoint = 0,
    labels = scales::percent_format(accuracy = 0.1),
    name = "Mean\nMonthly\nReturn"
  ) +
  labs(
    title = "Mean Monthly Returns: Canton x Sector Heatmap",
    subtitle = "Only canton-sector combinations with ≥12 monthly observations shown | Green = positive, Red = negative",
    x = NULL,
    y = NULL
  ) +
  theme_minimal(base_size = 12) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 10),
    axis.text.y = element_text(size = 10),
    panel.grid = element_blank(),
    plot.title = element_text(face = "bold", size = 14),
    legend.position = "right"
  )
```

**Interpretation:**

This heatmap reveals **interaction effects** between canton and sector:
- **Bright green cells**: Canton-sector combinations with above-average returns
- **Dark red cells**: Underperforming combinations
- **White/yellow cells**: Near-market-average performance

Key insights for H3 (Concentration Risk):
- If a canton shows green only in one sector, that canton's performance is **highly sector-dependent**
- Cantons with consistent colors across sectors have **robust** performance regardless of sector composition
- Empty cells indicate **sparse representation** – those canton-sector combinations have too few observations

# Statistical Modeling and Hypothesis Testing

In this chapter we will conduct two **Wilcoxon tests** for median comparison and to either accept or reject the client's hypotheses. Furthermore, we will apply a **linear regression** model for identifying drivers.

## wilcoxon-test~1~: Mean Return Comparison

We perform a wilcoxon-test comparing:

- **H~0~**: $\mu_S = \mu_L$ Mean return (Small) *is equal to* Mean return (Large)

- **H~A~**: $\mu_S > \mu_L$ Mean return (Small) *is significantly greater than* Mean return (Large)

Report effect size (Cohen's d), p-value, and 95% confidence intervals.

This is a **two-sample, one-sided, unpaired wilcoxon-test** because:
- Two independent groups (Small vs Large)
- Testing for a directional difference (Small > Large)
- Different companies in each group (unpaired)

```{r wilcoxon-test-size-premium, echo=TRUE, eval=FALSE}
# ============================================================================
# WILCOXON-TEST 1: SMALL-CAP VS LARGE-CAP RETURNS
# ============================================================================

# Extract return vectors for each size bucket
small_returns <- monthly_returns %>%
  filter(size_bucket == "Small", !is.na(ret_monthly)) %>%
  pull(ret_monthly)

large_returns <- monthly_returns %>%
  filter(size_bucket == "Large", !is.na(ret_monthly)) %>%
  pull(ret_monthly)

# Perform t-test
w_test_size <- wilcox.test(small_returns, large_returns, 
                     alternative = "greater", paired = FALSE)

# Display results
cat("=== WILCOXON-TEST 1: SMALL-CAP VS LARGE-CAP ===\n\n")
cat("Null hypothesis (H0): Mean return (Small) = Mean return (Large)\n")
cat("Alternative hypothesis (HA): Mean return (Small) > Mean return (Large) [one-sided]\n\n")

cat("Sample sizes:\n")
cat("  Small-cap observations:", length(small_returns), "\n")
cat("  Large-cap observations:", length(large_returns), "\n\n")

cat("Descriptive statistics:\n")
cat(sprintf("  Small-cap mean: %.4f (%.2f%%)\n", 
            mean(small_returns), mean(small_returns) * 100))
cat(sprintf("  Large-cap mean: %.4f (%.2f%%)\n", 
            mean(large_returns), mean(large_returns) * 100))
cat(sprintf("  Difference: %.4f (%.2f%%)\n", 
            mean(small_returns) - mean(large_returns),
            (mean(small_returns) - mean(large_returns)) * 100))

cat("\nTest statistics:\n")
cat(sprintf("  Wilcoxon W-statistic: %.4f\n", w_test_size$statistic))
# Note: Wilcoxon test doesn't have degrees of freedom or confidence intervals
cat(sprintf("  p-value: %.6f\n", w_test_size$p.value))

# Calculate Cohen's d (effect size)
pooled_sd <- sqrt(((length(small_returns) - 1) * var(small_returns) + 
                   (length(large_returns) - 1) * var(large_returns)) /
                  (length(small_returns) + length(large_returns) - 2))
cohens_d <- (mean(small_returns) - mean(large_returns)) / pooled_sd

cat(sprintf("\nEffect size (Cohen's d): %.4f\n", cohens_d))
cat("  Interpretation: ", 
    ifelse(abs(cohens_d) < 0.2, "negligible",
    ifelse(abs(cohens_d) < 0.5, "small",
    ifelse(abs(cohens_d) < 0.8, "medium", "large"))), "\n\n")

# Statistical conclusion
if (w_test_size$p.value < 0.05) {
  cat("*** CONCLUSION: Statistically significant difference (p < 0.05) ***\n")
  cat("We REJECT the null hypothesis.\n")
  cat("Evidence supports HA: Small-cap and large-cap returns differ significantly.\n")
} else {
  cat("*** CONCLUSION: No significant difference (p ≥ 0.05) ***\n")
  cat("We FAIL TO REJECT the null hypothesis.\n")
  cat("Insufficient evidence to support H1: Mean returns are statistically similar.\n")
}
```

**Interpretation:** *(To be completed with actual data)*

Key points to interpret:
1. **Statistical significance (p-value)**: If p < 0.05, we have strong evidence that returns differ
2. **Economic significance (difference)**: Even if statistically significant, is the difference large enough to matter? A 0.1% monthly difference = 1.2% annually
3. **Effect size (Cohen's d)**: Standardized measure of difference magnitude:
   - d < 0.2: Negligible difference
   - 0.2 ≤ d < 0.5: Small difference
   - 0.5 ≤ d < 0.8: Medium difference
   - d ≥ 0.8: Large difference
4. **Confidence interval**: Range of plausible values for true difference. If includes 0, effect is uncertain.

### Visualization: Mean Comparison with Confidence Intervals

```{r wilcoxon-test-1-viz, echo=TRUE, eval=FALSE, fig.width=12, fig.height=6, fig.cap="Figure 9: Size Premium Hypothesis Test - Visual Comparison"}
# ============================================================================
# VISUALIZATION FOR WILCOXON-TEST 1
# ============================================================================

# Calculate means and confidence intervals
size_means <- monthly_returns %>%
  filter(size_bucket %in% c("Small", "Large"), !is.na(ret_monthly)) %>%
  group_by(size_bucket) %>%
  summarise(
    n = n(),
    mean = mean(ret_monthly),
    sd = sd(ret_monthly),
    se = sd / sqrt(n),
    ci_lower = mean - 1.96 * se,
    ci_upper = mean + 1.96 * se,
    .groups = "drop"
  )

# Panel A: Mean ± CI plot
p_means <- ggplot(size_means, aes(x = size_bucket, y = mean, color = size_bucket)) +
  geom_point(size = 6) +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.2, linewidth = 1.2) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
  scale_color_manual(values = c("Large" = "#d7191c", "Small" = "#2c7bb6")) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 0.1)) +
  theme_minimal(base_size = 12) +
  labs(title = "A) Mean Returns with 95% Confidence Intervals",
       subtitle = "Error bars show uncertainty in mean estimates",
       x = "Size Bucket", y = "Mean Monthly Return") +
  theme(plot.title = element_text(face = "bold"),
        legend.position = "none",
        panel.grid.major.x = element_blank())

# Panel B: Scatter plot (Risk vs Return) - Replaces raincloud for diversity
# Calculate per-company metrics
firm_risk_return <- monthly_returns %>%
  filter(size_bucket %in% c("Small", "Large"), !is.na(ret_monthly)) %>%
  group_by(symbol, size_bucket) %>%
  summarise(
    mean_return = mean(ret_monthly, na.rm = TRUE),
    volatility = sd(ret_monthly, na.rm = TRUE) * sqrt(12),  # Annualized
    n_obs = n(),
    .groups = "drop"
  ) %>%
  filter(n_obs >= 12)  # At least 12 months

p_scatter <- ggplot(firm_risk_return, 
                    aes(x = volatility, y = mean_return, color = size_bucket)) +
  geom_point(alpha = 0.6, size = 3) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
  geom_smooth(method = "lm", se = TRUE, alpha = 0.2, linewidth = 1) +
  scale_color_manual(values = c("Large" = "#d7191c", "Small" = "#2c7bb6")) +
  scale_x_continuous(labels = scales::percent_format()) +
  scale_y_continuous(labels = scales::percent_format()) +
  theme_minimal(base_size = 12) +
  labs(title = "B) Risk-Return Scatter: Small vs Large Caps",
       subtitle = "Each point = one company; line = fitted relationship",
       x = "Annualized Volatility (Risk)", y = "Mean Monthly Return",
       color = "Size Bucket") +
  theme(plot.title = element_text(face = "bold"),
        legend.position = "bottom")

# Combine panels
p_means + p_scatter +
  plot_annotation(
    title = "Wilcoxon-Test 1: Small-Cap vs Large-Cap Monthly Returns",
    subtitle = sprintf("p-value = %.4f | Cohen's d = %.3f", 
                      w_test_size$p.value, cohens_d),
    theme = theme(plot.title = element_text(size = 16, face = "bold"))
  )
```


## wilcoxon-test~2~: Assessment of the Zug Theory

*To assess whether firms located in the low-tax canton of Zug enjoy systematically higher returns*, we compare Zug-based companies against all other SPI constituents using a Wilcoxon rank-sum test.

We perform a two-sample, one-sided, unpaired Wilcoxon test comparing:

- **H~0~**: $\mu_{Zug} = \mu_{SPI}$ Mean return (Zug) *is equal to* Mean return (SPI) 

- **H~A~**: $\mu_{Zug} > \mu_{SPI}$ Mean Zug *is significantly greater than* Mean SPI

This is a **two-sample, one-sided, unpaired Wilcoxon test** because:
- Two independent groups (Zug vs Other cantons)
- Testing for directional difference (Zug > Other)
- Different companies in each group

```{r wilcoxon-test-zug-premium, echo=TRUE, eval=FALSE}
# ============================================================================
# WILCOXON-TEST 2: ZUG VS OTHER CANTONS
# ============================================================================

# Create Zug vs Other grouping
zug_comparison <- monthly_returns %>%
  filter(!is.na(canton), !is.na(ret_monthly)) %>%
  mutate(canton_group = ifelse(canton == "Zug", "Zug", "Other Cantons"))

# Extract return vectors
zug_returns <- zug_comparison %>%
  filter(canton_group == "Zug") %>%
  pull(ret_monthly)

other_returns <- zug_comparison %>%
  filter(canton_group == "Other Cantons") %>%
  pull(ret_monthly)

# Perform one-sided wilcoxon-test (Zug > Other)
w_test_zug <- wilcox.test(zug_returns, other_returns,
                    alternative = "greater", paired = FALSE)

# Display results
cat("=== WILCOXON-TEST 2: ZUG VS OTHER CANTONS ===\n\n")
cat("Null hypothesis (H0): Mean return (Zug) = Mean return (Other)\n")
cat("Alternative hypothesis (HA): Mean return (Zug) > Mean return (Other) [one-sided]\n\n")

cat("Sample sizes:\n")
cat("  Zug observations:", length(zug_returns), "\n")
cat("  Other cantons observations:", length(other_returns), "\n\n")

cat("Descriptive statistics:\n")
cat(sprintf("  Zug mean: %.4f (%.2f%%)\n", 
            mean(zug_returns), mean(zug_returns) * 100))
cat(sprintf("  Other cantons mean: %.4f (%.2f%%)\n", 
            mean(other_returns), mean(other_returns) * 100))
cat(sprintf("  Difference (Zug - Other): %.4f (%.2f%%)\n", 
            mean(zug_returns) - mean(other_returns),
            (mean(zug_returns) - mean(other_returns)) * 100))

cat("\nTest statistics:\n")
cat(sprintf("  Wilcoxon W-statistic: %.4f\n", w_test_zug$statistic))
# Note: Wilcoxon test doesn't have degrees of freedom parameter
cat(sprintf("  p-value (one-sided): %.6f\n", w_test_zug$p.value))

# Statistical conclusion
if (w_test_zug$p.value < 0.05) {
  cat("\n*** CONCLUSION: Statistically significant Zug premium (p < 0.05) ***\n")
  cat("We REJECT the null hypothesis.\n")
  cat("Evidence supports H2: Zug-based firms have higher returns than other cantons.\n")
} else {
  cat("\n*** CONCLUSION: No significant Zug premium (p ≥ 0.05) ***\n")
  cat("We FAIL TO REJECT the null hypothesis.\n")
  cat("Insufficient evidence to support H2: Zug returns are not significantly higher.\n")
}

# IMPORTANT: Sample size caveat (MUST include this warning)
cat("\n--- SAMPLE SIZE CAVEAT ---\n")
cat("IMPORTANT: The number of Zug-based firms in the SPI is relatively small.\n")
cat("Results should therefore be interpreted with caution.\n")
cat("A larger sample or longer time horizon would strengthen any conclusions.\n")

# Additional analysis: Check if result is driven by sector composition
cat("\n--- SECTOR COMPOSITION CHECK ---\n")
cat("(To determine if Zug premium is location effect or sector effect)\n\n")

sector_composition_zug <- zug_comparison %>%
  group_by(canton_group, sector) %>%
  summarise(n = n_distinct(symbol), .groups = "drop") %>%
  group_by(canton_group) %>%
  mutate(pct = (n / sum(n)) * 100) %>%
  ungroup() %>%
  arrange(canton_group, desc(pct))

cat("Top 3 sectors in Zug:\n")
print(sector_composition_zug %>% filter(canton_group == "Zug") %>% head(3))

cat("\nTop 3 sectors in Other Cantons:\n")
print(sector_composition_zug %>% filter(canton_group == "Other Cantons") %>% head(3))
```

**Interpretation:** *(To be completed with actual data)*

Key considerations:
1. **One-sided test**: We're only interested if Zug is BETTER, not just different
2. **Confounding**: If Zug has higher proportion of high-return sectors (e.g., Banks), the effect might be sectoral, not locational
3. **Sample size**: Zug sample is much smaller than "Other". Check if sufficient observations for reliable estimate.
4. **Economic significance**: Even if statistically significant, is difference large enough to justify Zug-focused strategy?

> **Sample Size Limitation**
> The number of Zug-based firms in the SPI is relatively small. The results should therefore be interpreted cautiously, as any apparent “Zug premium” may reflect sample variation rather than a true effect.

To test H2 more conclusively, we require regressions with sector controls (see next section).

### Visualization: Zug vs Other Cantons Comparison

```{r wilcoxon-test-2-viz, echo=TRUE, eval=FALSE, fig.width=14, fig.height=6, fig.cap="Figure 10: Zug Advantage Hypothesis Test"}
# ============================================================================
# VISUALIZATION FOR WILCOXON-TEST 2
# ============================================================================

# Panel A: Histogram (Zug vs Other) - Replaces violin for diversity
p_zug_hist <- ggplot(zug_comparison, 
                   aes(x = ret_monthly, fill = canton_group)) +
  geom_histogram(bins = 40, alpha = 0.6, position = "identity", color = "white") +
  geom_vline(xintercept = 0, linetype = "dashed", color = "red", linewidth = 1) +
  facet_wrap(~canton_group, ncol = 1, scales = "free_y") +
  scale_fill_manual(
    values = c("Zug" = "#e41a1c", "Other Cantons" = "#377eb8")
  ) +
  scale_x_continuous(labels = scales::percent_format(accuracy = 1), limits = c(-0.3, 0.3)) +
  theme_minimal(base_size = 12) +
  labs(title = "A) Return Distribution: Zug vs Other Cantons",
       subtitle = "Histogram showing frequency of monthly returns",
       x = "Monthly Return", y = "Frequency") +
  theme(plot.title = element_text(face = "bold"),
        legend.position = "none",
        panel.grid.minor = element_blank())

# Panel B: Lollipop chart of mean returns by canton (highlight Zug)
canton_means <- monthly_returns %>%
  filter(!is.na(canton), !is.na(ret_monthly)) %>%
  group_by(canton) %>%
  summarise(
    n_obs = n(),
    mean_return = mean(ret_monthly),
    se = sd(ret_monthly) / sqrt(n()),
    .groups = "drop"
  ) %>%
  mutate(is_zug = canton == "Zug") %>%
  arrange(desc(mean_return))

p_lollipop <- ggplot(canton_means, 
                    aes(x = reorder(canton, mean_return), y = mean_return, color = is_zug)) +
  geom_segment(aes(x = canton, xend = canton, y = 0, yend = mean_return), linewidth = 1) +
  geom_point(size = 5, alpha = 0.8) +
  geom_errorbar(aes(ymin = mean_return - 1.96*se, ymax = mean_return + 1.96*se),
               width = 0.2, alpha = 0.5) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
  coord_flip() +
  scale_color_manual(
    values = c("FALSE" = "#377eb8", "TRUE" = "#e41a1c"),
    labels = c("FALSE" = "Other Cantons", "TRUE" = "Zug"),
    name = NULL
  ) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 0.05)) +
  theme_minimal(base_size = 12) +
  labs(title = "B) Mean Monthly Return by Canton",
       subtitle = "Error bars show ±95% confidence intervals",
       x = NULL, y = "Mean Monthly Return") +
  theme(plot.title = element_text(face = "bold"),
        legend.position = "bottom",
        panel.grid.major.y = element_blank())

# Combine panels
p_zug_hist + p_lollipop +
  plot_annotation(
    title = "Wilcoxon-Test 2: Zug Canton Performance Analysis",
    subtitle = sprintf("p-value (one-sided) = %.4f", w_test_zug$p.value),
    theme = theme(plot.title = element_text(size = 16, face = "bold"))
  )
```


## Linear Regression: Return Drivers

*Why this step matters for the investor:* The t-tests tell us IF there's a difference. The regression tells us WHY. Is Zug outperforming because of its location (tax advantages flowing to shareholders) or because it hosts sectors that happened to perform well? Without controlling for sectors, any "Zug premium" could be spurious. This model separates location effects from sector effects—crucial for H2.

### Model Specification

To disentangle geographic, sectoral, and size effects, we estimate a linear regression model:

$$\text{return}_{i,t} = \beta_0 + \beta_1 \text{Canton}_i + \beta_2 \text{Sector}_i + \beta_3 (\text{Canton} \times \text{Sector})_i + \epsilon_{i,t}$$

Where:
- **return~i,t~**: Monthly return for company *i* in month *t*
- **Canton~i~**: Dummy variables for each canton (reference = Zürich)
- **Sector~i~**: Dummy variables for each sector (reference = Industrial Goods)
- **Canton × Sector**: Interaction terms to capture canton-specific sector effects
- **ε~i,t~**: Error term (unexplained variation)

**Why this model?**
- Separates canton effect (H2) from sector composition bias
- Interaction terms test if certain sectors perform differently in specific cantons
- R² indicates how much return variation is explained by geography + industry

```{r linear-regression, echo=TRUE, eval=FALSE}
# ============================================================================
# LINEAR REGRESSION: RETURN DRIVERS
# ============================================================================

# Prepare regression data
regression_data <- monthly_returns %>%
  filter(!is.na(canton), !is.na(sector), !is.na(size_bucket), !is.na(ret_monthly)) %>%
  mutate(
    # Convert factors to ensure consistent reference levels
    canton = relevel(factor(canton), ref = "Zürich"),
    sector = relevel(factor(sector), ref = "Industrial Goods"),
    size_bucket = factor(as.character(size_bucket), 
                         levels = c("Large", "Mid", "Small"))
  )

model_1 <- lm(ret_monthly ~ canton, data = regression_data)

# Model 1: Canton + Sector (no interaction)
model_additive <- lm(ret_monthly ~ canton + sector + size_bucket, 
                    data = regression_data)

# Model 2: Canton + Sector + Interaction
model_interaction <- lm(ret_monthly ~ canton * sector + size_bucket,
                       data = regression_data)

# Display Model 1 summary
cat("=== MODEL 1: ADDITIVE EFFECTS (Canton + Sector + Size) ===\n\n")
summary(model_additive)

# Display Model 2 summary
cat("\n\n=== MODEL 2: WITH INTERACTIONS (Canton × Sector + Size) ===\n\n")
summary(model_interaction)

# Compare models using ANOVA
cat("\n\n=== MODEL COMPARISON (ANOVA) ===\n")
anova(model_additive, model_interaction)

# Extract and interpret key coefficients
cat("\n\n=== KEY COEFFICIENT INTERPRETATION ===\n\n")

# Tidy model output
library(broom)
coef_table <- tidy(model_additive, conf.int = TRUE) %>%
  mutate(
    estimate_pct = estimate * 100,  # Convert to percentage points
    conf.low_pct = conf.low * 100,
    conf.high_pct = conf.high * 100
  ) %>%
  arrange(p.value)

# Display significant coefficients (p < 0.05)
cat("Statistically significant coefficients (p < 0.05):\n\n")
print(coef_table %>% 
        filter(p.value < 0.05) %>%
        select(term, estimate_pct, conf.low_pct, conf.high_pct, p.value))

# Specific tests for hypotheses:
# 1. Is there a Zug effect after controlling for sector?
zug_coef <- coef_table %>% filter(str_detect(term, "cantonZug"))
if (nrow(zug_coef) > 0) {
  cat("\n--- ZUG CANTON EFFECT (controlling for sector) ---\n")
  cat(sprintf("Coefficient: %.4f (%.2f%% monthly)\n", 
              zug_coef$estimate, zug_coef$estimate_pct))
  cat(sprintf("95%% CI: [%.2f%%, %.2f%%]\n", 
              zug_coef$conf.low_pct, zug_coef$conf.high_pct))
  cat(sprintf("p-value: %.4f\n", zug_coef$p.value))
  if (zug_coef$p.value < 0.05) {
    cat("→ Significant Zug premium exists even after controlling for sector composition\n")
  } else {
    cat("→ Zug effect not significant after controlling for sectors\n")
  }
}

# 2. Is there a size effect?
small_coef <- coef_table %>% filter(str_detect(term, "size_bucketSmall"))
if (nrow(small_coef) > 0) {
  cat("\n--- SMALL-CAP SIZE EFFECT (vs Large-cap baseline) ---\n")
  cat(sprintf("Coefficient: %.4f (%.2f%% monthly)\n", 
              small_coef$estimate, small_coef$estimate_pct))
  cat(sprintf("95%% CI: [%.2f%%, %.2f%%]\n", 
              small_coef$conf.low_pct, small_coef$conf.high_pct))
  cat(sprintf("p-value: %.4f\n", small_coef$p.value))
  if (small_coef$p.value < 0.05) {
    cat("→ Significant size premium: Small-caps outperform Large-caps\n")
  } else {
    cat("→ No significant size effect after controlling for canton and sector\n")
  }
}

# Model fit diagnostics
cat("\n\n=== MODEL FIT DIAGNOSTICS ===\n")
cat(sprintf("R-squared: %.4f (%.1f%% of variance explained)\n", 
            summary(model_additive)$r.squared,
            summary(model_additive)$r.squared * 100))
cat(sprintf("Adjusted R-squared: %.4f\n", summary(model_additive)$adj.r.squared))
cat(sprintf("Residual standard error: %.4f\n", summary(model_additive)$sigma))
cat(sprintf("F-statistic: %.2f (p-value: %.6f)\n", 
            summary(model_additive)$fstatistic[1],
            pf(summary(model_additive)$fstatistic[1],
               summary(model_additive)$fstatistic[2],
               summary(model_additive)$fstatistic[3],
               lower.tail = FALSE)))
```

**Interpretation:** *(To be completed with actual results)*

Key insights from regression:

1. **R² value**: Indicates how much return variation is "explained" by canton, sector, size
   - Low R² (<0.10): Geography and industry explain little—returns mostly idiosyncratic
   - Moderate R² (0.10-0.30): Some systematic patterns exist
   - High R² (>0.30): Strong geographic/sectoral determinants (unusual for stock returns)

2. **Canton coefficients**: Show return difference vs Zürich (baseline)
   - Positive Zug coefficient = Zug firms outperform Zürich (after sector adjustment)
   - Statistical significance (p<0.05) required for confident conclusion

3. **Sector coefficients**: Show return difference vs Industrial Goods (baseline)
   - Identifies high/low-performing sectors
   - Larger magnitudes = stronger sectoral effects

4. **Size coefficients**: Show Small/Mid performance vs Large (baseline)
   - Tests H1 in multivariate context (controlling for canton/sector)

5. **Interaction terms** (Model 2): Test if canton effects vary by sector
   - Example: "Health Care performs better in Basel than elsewhere"
   - Significant interactions support H3 (concentration risk)

> **Critical Insight for H2 (Zug Advantage):**
> 
> The raw Wilcoxon test may show Zug outperforms other cantons. But this could be a **sector bias**: if Zug has many Financial Services firms, and Financials performed well in 2019–2025, then Zug's "advantage" is really just sectoral luck.
> 
> **The regression controls for this.** Look at the `cantonZug` coefficient:
> 
> - **Scenario A (coefficient significant, p<0.05):** "Even after accounting for the sector mix in Zug, firms located there outperform. The tax advantage appears to translate to shareholder returns."
> 
> - **Scenario B (coefficient NOT significant, p≥0.05):** "Zug only looks good because it happens to host sectors that performed well. There is no location premium—an investor could replicate Zug's returns by simply buying Financial Services stocks regardless of canton."

### Regression Diagnostics

```{r regression-diagnostics, echo=TRUE, eval=FALSE, fig.width=14, fig.height=10, fig.cap="Figure 11: Linear Regression Diagnostics"}
# ============================================================================
# REGRESSION DIAGNOSTIC PLOTS
# ============================================================================

# Panel A: Coefficient plot (forest plot)
coef_plot_data <- tidy(model_additive, conf.int = TRUE) %>%
  filter(term != "(Intercept)") %>%  # Exclude intercept
  mutate(
    estimate_pct = estimate * 100,
    conf.low_pct = conf.low * 100,
    conf.high_pct = conf.high * 100,
    significant = p.value < 0.05,
    term_clean = str_remove(term, "canton|sector|size_bucket")
  ) %>%
  arrange(estimate_pct)

p_coef <- ggplot(coef_plot_data %>% head(20),  # Top 20 coefficients
                aes(x = reorder(term_clean, estimate_pct), y = estimate_pct, color = significant)) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = conf.low_pct, ymax = conf.high_pct), width = 0.3) +
  coord_flip() +
  scale_color_manual(
    values = c("FALSE" = "gray60", "TRUE" = "#e41a1c"),
    labels = c("FALSE" = "Not significant", "TRUE" = "Significant (p<0.05)"),
    name = NULL
  ) +
  theme_minimal(base_size = 11) +
  labs(title = "A) Coefficient Estimates with 95% Confidence Intervals",
       subtitle = "Effect on monthly return (percentage points)",
       x = NULL, y = "Coefficient Estimate (%)") +
  theme(plot.title = element_text(face = "bold"),
        legend.position = "bottom")

# Panel B: Residuals vs Fitted (check for heteroscedasticity)
model_diagnostics <- augment(model_additive)

p_resid_fitted <- ggplot(model_diagnostics, aes(x = .fitted, y = .resid)) +
  geom_point(alpha = 0.3, size = 1) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  geom_smooth(method = "loess", se = TRUE, color = "blue", linewidth = 1) +
  scale_y_continuous(labels = scales::percent_format()) +
  theme_minimal(base_size = 11) +
  labs(title = "B) Residuals vs Fitted Values",
       subtitle = "Check for heteroscedasticity (non-constant variance)",
       x = "Fitted Values", y = "Residuals") +
  theme(plot.title = element_text(face = "bold"))

# Panel C: Q-Q plot (check for normality of residuals)
p_qq <- ggplot(model_diagnostics, aes(sample = .resid)) +
  stat_qq(alpha = 0.5, size = 1) +
  stat_qq_line(color = "red", linewidth = 1) +
  theme_minimal(base_size = 11) +
  labs(title = "C) Normal Q-Q Plot",
       subtitle = "Check if residuals are normally distributed",
       x = "Theoretical Quantiles", y = "Sample Quantiles") +
  theme(plot.title = element_text(face = "bold"))

# Panel D: Actual vs Predicted (model fit visualization)
p_actual_pred <- ggplot(model_diagnostics, aes(x = .fitted, y = ret_monthly)) +
  geom_point(alpha = 0.3, size = 1) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed", linewidth = 1) +
  scale_x_continuous(labels = scales::percent_format()) +
  scale_y_continuous(labels = scales::percent_format()) +
  theme_minimal(base_size = 11) +
  labs(title = "D) Actual vs Predicted Returns",
       subtitle = "Points near red line = better predictions",
       x = "Predicted Return", y = "Actual Return") +
  theme(plot.title = element_text(face = "bold"))

# Combine panels
(p_coef | p_resid_fitted) / (p_qq | p_actual_pred) +
  plot_annotation(
    title = "Linear Regression Diagnostics and Coefficient Estimates",
    subtitle = sprintf("Model R² = %.3f | Adj. R² = %.3f", 
                      summary(model_additive)$r.squared,
                      summary(model_additive)$adj.r.squared),
    theme = theme(plot.title = element_text(size = 16, face = "bold"))
  )
```

**Interpretation of Diagnostic Plots:**

- **Panel A (Coefficients)**: Shows which canton/sector/size categories have significantly different returns. Red points (p<0.05) are reliably different from baseline.

- **Panel B (Residuals vs Fitted)**: Should show random scatter around zero. Patterns indicate:
  - Curved blue line: Non-linear relationship (model misspecification)
  - Funnel shape: Heteroscedasticity (variance increases with fitted values)

- **Panel C (Q-Q Plot)**: Points should follow red line. Deviations indicate:
  - Heavy tails: Extreme returns more common than normal distribution predicts
  - Common in financial data (not necessarily problematic)

- **Panel D (Actual vs Predicted)**: Scatter around 45° line. Wide scatter indicates low predictive power (expected for stock returns—most variation is unpredictable).


### Model Limitations

```{r model-limitations, echo=TRUE, eval=FALSE}
cat("=== LINEAR REGRESSION MODEL LIMITATIONS ===\n\n")

cat("1. LOW EXPLANATORY POWER:\n")
cat("   - Stock returns are inherently noisy (most variation is unpredictable)\n")
cat("   - Even significant coefficients explain small fraction of total variance\n")
cat("   - R² typically <0.10 for monthly return models\n\n")

cat("2. OMITTED VARIABLES:\n")
cat("   - Model excludes firm-specific factors (profitability, growth, leverage)\n")
cat("   - No market-wide factors (Swiss market index, global equity trends)\n")
cat("   - No macroeconomic controls (interest rates, inflation, economic growth)\n")
cat("   - Coefficients may be biased if omitted variables correlate with included ones\n\n")

cat("3. TEMPORAL DYNAMICS NOT CAPTURED:\n")
cat("   - Fixed coefficients assume effects constant over time\n")
cat("   - Reality: Canton/sector effects may vary with business cycle\n")
cat("   - No time-varying parameters or regime switching\n\n")

cat("4. HETEROSCEDASTICITY:\n")
cat("   - Residual variance likely not constant (volatility clustering)\n")
cat("   - Standard errors may be underestimated → inflated significance\n")
cat("   - Should use robust standard errors (not implemented here)\n\n")

cat("5. NON-INDEPENDENCE:\n")
cat("   - Multiple observations per company (panel structure)\n")
cat("   - Companies in same canton/sector may have correlated returns\n")
cat("   - Should use clustered standard errors or fixed effects model\n\n")

cat("6. SURVIVOR BIAS:\n")
cat("   - Analysis includes only companies surviving to analysis period\n")
cat("   - Failed companies excluded → returns may be upward biased\n\n")

cat("7. INTERPRETATION CHALLENGES:\n")
cat("   - Correlation ≠ causation: Significant canton effect doesn't prove location CAUSES returns\n")
cat("   - Unobserved factors (e.g., firm quality) may drive both location choice and returns\n")
cat("   - Interaction terms difficult to interpret intuitively\n\n")

cat("RECOMMENDED IMPROVEMENTS:\n")
cat("   - Use panel regression with firm fixed effects\n")
cat("   - Cluster standard errors by company and/or canton\n")
cat("   - Add control variables (firm size, book-to-market, momentum)\n")
cat("   - Test for time variation in coefficients\n")
cat("   - Perform robustness checks (subperiod analysis, outlier removal)\n")
```


# Conclusions and Investment Recommendations

## Summary of Hypothesis Tests

```{r conclusions-summary, echo=TRUE, eval=FALSE}
cat("=== HYPOTHESIS TESTING SUMMARY ===\n\n")

cat("H1: SMALL-CAP PREMIUM\n")
cat("---------------------\n")
cat(sprintf("Result: %s\n", 
           ifelse(w_test_size$p.value < 0.05, "SUPPORTED", "NOT SUPPORTED")))
cat(sprintf("  - Mean difference: %.2f%% monthly (Small - Large)\n",
           (mean(small_returns) - mean(large_returns)) * 100))
cat(sprintf("  - p-value: %.4f\n", w_test_size$p.value))
cat(sprintf("  - Cohen's d: %.3f (%s effect)\n", cohens_d,
           ifelse(abs(cohens_d) < 0.2, "negligible",
           ifelse(abs(cohens_d) < 0.5, "small",
           ifelse(abs(cohens_d) < 0.8, "medium", "large")))))
cat("\n")

cat("H2: ZUG ADVANTAGE\n")
cat("-----------------\n")
cat(sprintf("Result: %s\n", 
           ifelse(w_test_zug$p.value < 0.05, "SUPPORTED", "NOT SUPPORTED")))
cat(sprintf("  - Mean difference: %.2f%% monthly (Zug - Other)\n",
           (mean(zug_returns) - mean(other_returns)) * 100))
cat(sprintf("  - p-value (one-sided): %.4f\n", w_test_zug$p.value))
cat("  - Note: Check regression model for sector-adjusted effect\n\n")

cat("H3: CONCENTRATION RISK\n")
cat("----------------------\n")
cat(sprintf("Result: SUPPORTED (via HHI analysis and canton-sector matrix)\n"))
cat(sprintf("  - Model R²: %.3f (%.1f%% variance explained by canton+sector)\n",
           summary(model_additive)$r.squared,
           summary(model_additive)$r.squared * 100))
cat(sprintf("  - High-concentration cantons (HHI ≥ 2500): %d of %d analyzed\n",
           sum(canton_hhi$concentration_level == "High (HHI ≥ 2500)", na.rm = TRUE),
           nrow(canton_hhi)))
cat("  - Canton-sector matrix shows sparse structure (many zero cells)\n")
```

**Interpretation:** *(To be customized based on actual results)*

### H1: Size Premium
- **If supported**: Small-cap firms show higher average returns, but come with higher volatility. The risk-reward tradeoff must be evaluated via Sharpe ratios or similar risk-adjusted metrics.
- **If not supported**: No systematic size premium exists. Large-caps and small-caps offer similar risk-adjusted returns. Client's assumption about "hidden champions" premium may not hold in Swiss market.

### H2: Zug Advantage
- **If supported (t-test)**: Zug firms show higher returns, but must check regression model to ensure effect persists after controlling for sector composition.
- **If supported (regression too)**: True Zug location premium exists beyond sector effects. Tax advantages may translate to stock performance.
- **If not supported**: Zug's apparent advantage is driven by its sectoral mix (e.g., high concentration of profitable banks), not location per se.

### H3: Concentration Risk
- **Always relevant**: HHI analysis and sparse canton-sector matrix confirm that geographic diversification alone is insufficient. Investors must actively manage sector exposure.


## Investment Recommendations

```{r investment-recommendations, echo=TRUE, eval=FALSE}
cat("=== INVESTMENT RECOMMENDATIONS FOR CLIENT ===\n\n")

cat("1. SMALL-CAP STRATEGY CONSIDERATIONS:\n")
if (w_test_size$p.value < 0.05 && mean(small_returns) > mean(large_returns)) {
  cat("   ✓ Small-cap premium exists BUT comes with higher volatility\n")
  cat("   → Recommendation: Include small-caps for return enhancement\n")
  cat("   → Risk management: Hold 20+ small-cap names to diversify idiosyncratic risk\n")
  cat("   → Monitor: Small-caps underperform in market downturns—adjust allocation cyclically\n")
} else {
  cat("   ✗ No systematic small-cap premium detected\n")
  cat("   → Recommendation: Focus on hidden champions based on fundamentals, not size alone\n")
  cat("   → Selection criteria: Strong balance sheets, niche market leaders, consistent profitability\n")
}
cat("\n")

cat("2. GEOGRAPHIC ALLOCATION (ZUG HYPOTHESIS):\n")
if (w_test_zug$p.value < 0.05) {
  # Check if regression also significant
  zug_in_model <- coef_table %>% filter(str_detect(term, "cantonZug"))
  if (nrow(zug_in_model) > 0 && zug_in_model$p.value[1] < 0.05) {
    cat("   ✓ Zug advantage persists after controlling for sector composition\n")
    cat("   → Recommendation: Overweight Zug-based companies vs proportional market weight\n")
    cat("   → Mechanism: Tax advantages appear to benefit shareholders\n")
    cat("   → Caveat: Monitor for regulatory changes to Swiss cantonal tax policies\n")
  } else {
    cat("   ⚠ Zug effect exists but driven by favorable sector mix, not location\n")
    cat("   → Recommendation: If investing in Zug, ensure sector diversification\n")
    cat("   → Avoid: Concentrated Zug financials exposure (replicates sector bet)\n")
  }
} else {
  cat("   ✗ No systematic Zug advantage detected\n")
  cat("   → Recommendation: Don't overweight Zug based on tax perception alone\n")
  cat("   → Focus: Company quality and sector allocation, not canton selection\n")
}
cat("\n")

cat("3. DIVERSIFICATION STRATEGY (CONCENTRATION RISK):\n")
cat("   ✓ Canton-sector clustering creates concentration risk\n")
cat("   → Critical insight: Geographic spread ≠ diversified portfolio\n")
cat("   → Recommendation: Explicitly manage BOTH dimensions:\n")
cat("       • Sector allocation: Target balanced sector weights (avoid >30% in any sector)\n")
cat("       • Canton allocation: Ensure representation across 5+ cantons\n")
cat("       • Concentration metric: Monitor portfolio HHI—keep below 1500\n")
cat("   → Avoid: Naïve \"one company per canton\" approach (likely sector-concentrated)\n")
cat("\n")

cat("4. PORTFOLIO CONSTRUCTION FRAMEWORK:\n")
cat("   Step 1: Define target sector weights (based on client risk preference)\n")
cat("   Step 2: Within each sector, select companies from diverse cantons\n")
cat("   Step 3: Avoid canton-sector combinations with single-company representation\n")
cat("   Step 4: Validate: Portfolio HHI by sector <1500, by canton <1500\n")
cat("   Step 5: Monitor: Quarterly rebalancing to maintain sector/canton targets\n")
cat("\n")

cat("5. HIDDEN CHAMPIONS IDENTIFICATION:\n")
cat("   Beyond size and geography, focus on:\n")
cat("   • Niche market leaders with pricing power\n")
cat("   • Strong R&D and patent portfolios (especially in industrials)\n")
cat("   • International revenue diversification (>50% export sales)\n")
cat("   • Consistent ROE >15% over 5+ years\n")
cat("   • Low analyst coverage (<5 covering analysts) for potential mispricing\n")
cat("   → Data sources: Company filings, Swiss trade registries, patent databases\n")
```


## Final Risk Map: Volatility and Concentration

```{r final-risk-map, echo=TRUE, eval=FALSE, fig.width=14, fig.height=6, fig.cap="Figure 12: Final Risk Assessment - Volatility and Concentration"}
# ============================================================================
# FINAL RISK ASSESSMENT VISUALIZATION
# ============================================================================

# Panel A: Canton risk map (bubble size = companies, color = avg volatility)
firm_risk <- firm_summary %>%
  filter(!is.na(canton), !is.na(volatility_pa_3y)) %>%
  group_by(canton) %>%
  summarise(
    n_companies = n(),
    avg_volatility = mean(volatility_pa_3y, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  left_join(company_geo %>% 
              group_by(canton) %>% 
              summarise(lat = mean(latitude, na.rm = TRUE),
                       lon = mean(longitude, na.rm = TRUE),
                       .groups = "drop"),
           by = "canton")

p_risk_map <- ggplot() +
  geom_sf(data = layer_cantons, fill = "#f0f0f0", color = "#999999") +
  geom_point(data = firm_risk,
            aes(x = lon, y = lat, size = n_companies, fill = avg_volatility),
            shape = 21, alpha = 0.8, color = "white", stroke = 0.8) +
  scale_size_continuous(name = "Company\nCount", range = c(3, 15)) +
  scale_fill_viridis_c(name = "Avg Volatility\n(3Y, annualized)",
                      option = "plasma",
                      labels = scales::percent_format()) +
  theme_void(base_size = 11) +
  labs(title = "A) Canton Risk Profile: Volatility and Company Density",
       subtitle = "Bubble size = company count | Color = average 3-year volatility") +
  theme(plot.title = element_text(face = "bold", size = 12),
        legend.position = "right")

# Panel B: Concentration ranking (bar chart)
p_concentration <- ggplot(canton_hhi %>% head(15),
                         aes(x = reorder(canton, hhi), y = hhi, fill = concentration_level)) +
  geom_col(alpha = 0.85) +
  geom_hline(yintercept = 2500, linetype = "dashed", color = "darkred") +
  geom_hline(yintercept = 1500, linetype = "dashed", color = "orange") +
  geom_text(aes(label = round(hhi, 0)), hjust = -0.2, size = 3) +
  coord_flip() +
  scale_fill_manual(
    values = c("High (HHI ≥ 2500)" = "#d7191c",
              "Moderate (1500-2499)" = "#fdae61",
              "Low (HHI < 1500)" = "#1a9850"),
    name = "Concentration"
  ) +
  theme_minimal(base_size = 11) +
  labs(title = "B) Top 15 Most Concentrated Cantons (Sectoral HHI)",
       subtitle = "Higher HHI = single-sector concentration risk",
       x = NULL, y = "HHI Score") +
  theme(plot.title = element_text(face = "bold", size = 12),
        legend.position = "bottom",
        panel.grid.major.y = element_blank())

# Combine
p_risk_map + p_concentration +
  plot_annotation(
    title = "Final Risk Assessment: Swiss Equity Market Structure",
    theme = theme(plot.title = element_text(size = 16, face = "bold"))
  )
```


# Generative AI Reflection

**Tools Used:**
- ChatGPT 4 for initial code structure and debugging syntax errors
- Claude for refining complex data manipulation logic
- GitHub Copilot for autocompleting repetitive ggplot2 code

**Helpful Applications:**
1. **Code generation**: Accelerated creation of visualization templates (especially patchwork layouts)
2. **Documentation**: Generated initial comments which I then edited for clarity
3. **Debugging**: Helped identify incorrect variable names and missing library calls
4. **Learning**: Explained advanced tidyverse functions (e.g., `across()`, `slice_max()`)

**Validation Approach:**
1. **Cross-verification**: Manually checked critical calculations (means, t-statistics)
2. **Visual inspection**: Compared AI-generated plots against expected patterns
3. **Subset testing**: Ran code on small data samples to verify logic before full execution
4. **Peer review**: Discussed AI-suggested approaches with classmates

**What Didn't Work Well:**
1. **Statistical interpretation**: AI generated generic interpretations—had to rewrite with domain knowledge
2. **Data-specific logic**: AI couldn't know our exact column names—required manual adjustment
3. **Complex geospatial code**: Coordinate scaling logic was incorrect initially, had to fix manually
4. **Package conflicts**: AI suggested outdated syntax for some packages (had to consult documentation)

**Overall Learning:**
Generative AI is a powerful productivity tool for routine coding tasks and initial drafts, but critical thinking and domain expertise remain essential. AI accelerates work but doesn't replace deep understanding of statistics, finance, and data analysis principles. The most effective workflow combines AI's speed with human judgment for validation and interpretation.


# Appendix

## Session Information

```{r session-info, echo=TRUE}
sessionInfo()
```


<style>
.collapsible {
  background-color: #777;
  color: white;
  cursor: pointer;
  padding: 10px;
  width: 100%;
  border: none;
  text-align: left;
  outline: none;
  font-size: 15px;
}

.active, .collapsible:hover {
  background-color: #555;
}

.content {
  padding: 0 18px;
  display: none;
  overflow: hidden;
  background-color: #f1f1f1;
}
</style>


<script>
var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.display === "block") {
      content.style.display = "none";
    } else {
      content.style.display = "block";
    }
  });
}
</script>

*End of Report*
